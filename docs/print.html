<!DOCTYPE HTML>
<html lang="en" class="light" dir="ltr">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>Crafteo - Kubernetes Training</title>
        <meta name="robots" content="noindex">


        <!-- Custom HTML head -->
        
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff">

        <link rel="icon" href="favicon.svg">
        <link rel="shortcut icon" href="favicon.png">
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        <link rel="stylesheet" href="css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="highlight.css">
        <link rel="stylesheet" href="tomorrow-night.css">
        <link rel="stylesheet" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->

    </head>
    <body class="sidebar-visible no-js">
    <div id="body-container">
        <!-- Provide site root to javascript -->
        <script>
            var path_to_root = "";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('light')
            html.classList.add(theme);
            var body = document.querySelector('body');
            body.classList.remove('no-js')
            body.classList.add('js');
        </script>

        <input type="checkbox" id="sidebar-toggle-anchor" class="hidden">

        <!-- Hide / unhide sidebar before it is displayed -->
        <script>
            var body = document.querySelector('body');
            var sidebar = null;
            var sidebar_toggle = document.getElementById("sidebar-toggle-anchor");
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            } else {
                sidebar = 'hidden';
            }
            sidebar_toggle.checked = sidebar === 'visible';
            body.classList.remove('sidebar-visible');
            body.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded affix "><a href="welcome.html">Crafteo - Kubernetes Training</a></li><li class="chapter-item expanded affix "><li class="part-title">Introduction</li><li class="chapter-item expanded "><a href="intro/pod.html"><strong aria-hidden="true">1.</strong> Our first Pod</a></li><li class="chapter-item expanded "><a href="intro/deployment.html"><strong aria-hidden="true">2.</strong> Deployment</a></li><li class="chapter-item expanded "><a href="intro/service.html"><strong aria-hidden="true">3.</strong> Service</a></li><li class="chapter-item expanded "><a href="intro/logs-exec-others.html"><strong aria-hidden="true">4.</strong> Logs, exec and other operations</a></li><li class="chapter-item expanded affix "><li class="part-title">Controllers</li><li class="chapter-item expanded "><a href="controllers/deployments.html"><strong aria-hidden="true">5.</strong> Deployments & ReplicaSets</a></li><li class="chapter-item expanded "><a href="controllers/jobs.html"><strong aria-hidden="true">6.</strong> Jobs & CronJobs</a></li><li class="chapter-item expanded "><a href="controllers/daemonsets.html"><strong aria-hidden="true">7.</strong> DaemonSets</a></li><li class="chapter-item expanded "><a href="controllers/statefulsets.html"><strong aria-hidden="true">8.</strong> StatefulSets</a></li><li class="chapter-item expanded affix "><li class="part-title">Services & Networking</li><li class="chapter-item expanded "><a href="services/nodeport.html"><strong aria-hidden="true">9.</strong> NodePort</a></li><li class="chapter-item expanded "><a href="services/loadbalancers.html"><strong aria-hidden="true">10.</strong> LoadBalancer</a></li><li class="chapter-item expanded "><a href="services/ingresses.html"><strong aria-hidden="true">11.</strong> Ingresses and Ingress Controllers</a></li><li class="chapter-item expanded "><a href="services/externalname.html"><strong aria-hidden="true">12.</strong> ExternalName Services</a></li><li class="chapter-item expanded "><a href="services/advanced.html"><strong aria-hidden="true">13.</strong> Advanced: Endpoints, Headless Service, Service discovery...</a></li><li class="chapter-item expanded affix "><li class="part-title">Configs & Secrets</li><li class="chapter-item expanded "><a href="config/configmap.html"><strong aria-hidden="true">14.</strong> ConfigMap</a></li><li class="chapter-item expanded "><a href="config/secret.html"><strong aria-hidden="true">15.</strong> Secret</a></li><li class="chapter-item expanded affix "><li class="part-title">Volumes: Persistent & Ephemeral</li><li class="chapter-item expanded "><a href="volumes/pvc.html"><strong aria-hidden="true">16.</strong> Persistent Volumes: PVC & PV</a></li><li class="chapter-item expanded "><a href="volumes/emptydir.html"><strong aria-hidden="true">17.</strong> Ephemeral Volumes: emptyDir</a></li><li class="chapter-item expanded affix "><li class="part-title">Deployment tools</li><li class="chapter-item expanded "><a href="deploy-tools/helm-intro.html"><strong aria-hidden="true">18.</strong> Helm: introduction</a></li><li class="chapter-item expanded "><a href="deploy-tools/helm-advanced.html"><strong aria-hidden="true">19.</strong> Helm: advanced</a></li><li class="chapter-item expanded "><a href="deploy-tools/kustomize.html"><strong aria-hidden="true">20.</strong> Kustomize</a></li><li class="chapter-item expanded affix "><li class="part-title">Deploy in Production</li><li class="chapter-item expanded "><a href="production/tls.html"><strong aria-hidden="true">21.</strong> Ingress & TLS / HTTPS</a></li><li class="chapter-item expanded "><a href="production/resources.html"><strong aria-hidden="true">22.</strong> Resources Requests & Limits</a></li><li class="chapter-item expanded "><a href="production/horizontal-scaling.html"><strong aria-hidden="true">23.</strong> Horizontal Scalability</a></li><li class="chapter-item expanded "><a href="production/vertical-scaling.html"><strong aria-hidden="true">24.</strong> Vertical Scalability</a></li><li class="chapter-item expanded "><a href="production/probes.html"><strong aria-hidden="true">25.</strong> Liveness, Readiness and Startup Probes</a></li><li class="chapter-item expanded "><a href="production/pdb.html"><strong aria-hidden="true">26.</strong> Pod Disruption Budget (PDB)</a></li><li class="chapter-item expanded affix "><li class="part-title">Scheduling</li><li class="chapter-item expanded "><a href="scheduling/intro.html"><strong aria-hidden="true">27.</strong> Introduction</a></li><li class="chapter-item expanded "><a href="scheduling/inter-pod-affinity.html"><strong aria-hidden="true">28.</strong> Inter-pod affinity</a></li><li class="chapter-item expanded "><a href="scheduling/topology-spread-constraint.html"><strong aria-hidden="true">29.</strong> Topology Spread Constraint</a></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle"></div>
        </nav>

        <!-- Track and set sidebar scroll position -->
        <script>
            var sidebarScrollbox = document.querySelector('#sidebar .sidebar-scrollbox');
            sidebarScrollbox.addEventListener('click', function(e) {
                if (e.target.tagName === 'A') {
                    sessionStorage.setItem('sidebar-scroll', sidebarScrollbox.scrollTop);
                }
            }, { passive: true });
            var sidebarScrollTop = sessionStorage.getItem('sidebar-scroll');
            sessionStorage.removeItem('sidebar-scroll');
            if (sidebarScrollTop) {
                // preserve sidebar scroll position when navigating via links within sidebar
                sidebarScrollbox.scrollTop = sidebarScrollTop;
            } else {
                // scroll sidebar to current active section when navigating via "next/previous chapter" buttons
                var activeSection = document.querySelector('#sidebar .active');
                if (activeSection) {
                    activeSection.scrollIntoView({ block: 'center' });
                }
            }
        </script>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky">
                    <div class="left-buttons">
                        <label id="sidebar-toggle" class="icon-button" for="sidebar-toggle-anchor" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </label>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title">Crafteo - Kubernetes Training</h1>

                    <div class="right-buttons">
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>
                        <a href="https://github.com/PierreBeucher/k8s.training.crafteo.io" title="Git repository" aria-label="Git repository">
                            <i id="git-repository-button" class="fa fa-github"></i>
                        </a>

                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script>
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <h1 id="crafteo---kubernetes-training"><a class="header" href="#crafteo---kubernetes-training">Crafteo - Kubernetes Training</a></h1>
<p>Welcome on Kubernetes training exercise book ! Follow link from menu to access modules.</p>
<p>More info on <a href="https://crafteo.io">crafteo.io</a></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="our-first-pod-"><a class="header" href="#our-first-pod-">Our first Pod !</a></h1>
<p>Let's create our first Pod. </p>
<p>Start by creating a <em>Namespace</em>. A Namespaces is like an independent place where our Pod will be created in Kubernetes cluster to avoid conflict with other objects:</p>
<pre><code class="language-sh">kubectl create namespace &lt;your_ns&gt;
</code></pre>
<p>Then create a Pod in our namespace (<code>-n</code> flag specify namespace to use):</p>
<pre><code class="language-sh">kubectl -n &lt;your_ns&gt; run mypod --image us-docker.pkg.dev/google-samples/containers/gke/whereami:v1.2.21 --port 8080
</code></pre>
<p>It's like <code>docker run</code> for Kubernetes. </p>
<p>To reach your Pod externally, use:</p>
<pre><code class="language-sh">kubectl -n &lt;your_ns&gt; port-forward pod/mypod --address 0.0.0.0 8081:8080
</code></pre>
<p>Your Pod is now reachable on port 8081. Try to reach it locally or via browser:</p>
<pre><code class="language-sh">curl YOU.training.crafteo.io:8081
</code></pre>
<p>This is the equivalent running Docker command below, except our Pod (and container) are not reachable locally but only within the Kubernetes cluster. </p>
<pre><code class="language-sh">docker run -d -p 8081:81 us-docker.pkg.dev/google-samples/containers/gke/whereami:v1.2.21
</code></pre>
<hr />
<p>In reality, <code>kubectl run</code> is rarely used, except for debug purposes. Most of the time <em>YAML Manifests</em> are used to <strong>describe</strong> Kubernetes objects.</p>
<p>Create a Pod using YAML manifest with:</p>
<pre><code class="language-sh">kubectl -n &lt;your_ns&gt; apply -f intro/pod.yml
</code></pre>
<p>Now, use <code>kubectl</code> commands to:</p>
<ul>
<li>Get all Pods </li>
<li>Port-forward pod created via <code>intro/pod.yml</code></li>
<li>Delete both Pods with <code>kubectl delete</code></li>
</ul>
<hr />
<p>By default <code>kubectl</code> fetch Pods (and other objects) in the <code>default</code> namespace. You can change this behavior with:</p>
<pre><code class="language-sh">kubectl config set-context --current --namespace pierre
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="deployment-manage-multiple-pods"><a class="header" href="#deployment-manage-multiple-pods">Deployment: manage multiple Pods</a></h1>
<p>Deployment manages multiple Pods.</p>
<ul>
<li>Create a Deployment with <code>kubectl</code> from manifest <code>intro/deployment.yml</code>. 
<ul>
<li>Use <code>kubectl --help</code>, a search engine or IA if needed</li>
<li>Command looks like <code>kubectl apply ...</code></li>
</ul>
</li>
<li>List existing Deployments with <code>kubectl get</code></li>
<li>Restart the Pod created by our Deployment
<ul>
<li>There's no <code>restart</code> command, find another way</li>
</ul>
</li>
<li>Update your Deployment to have 3 replicas of our Pod</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="service-accessing-our-pods"><a class="header" href="#service-accessing-our-pods">Service: accessing our Pods</a></h1>
<p>Manage Services ith manifests and access a Pod via Service</p>
<ul>
<li>Create a Service with <code>kubectl</code> using manifest <code>intro/service.yml</code>
<ul>
<li>Reminder: <code>kubectl apply -f ...</code></li>
</ul>
</li>
<li>Use <code>kubectl port-forward</code> to expose the service locally and test access
<ul>
<li>Port forwarding will <em>forward</em> a local port yo tour Pod. Use <code>curl localhost:PORT</code> or equivalent. </li>
<li><code>kubectl port-forward --help</code> or checkout official doc</li>
</ul>
</li>
<li>How does a Service identify which Pods to serve ?</li>
</ul>
<hr />
<ul>
<li>Delete Pods, Deployment and Service with a single <code>kubectl</code> command</li>
<li>Re-apply our Deployment, Service and Pod with a single command</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="logs-exec-and-other-basic-operations"><a class="header" href="#logs-exec-and-other-basic-operations">Logs, exec and other basic operations</a></h1>
<p>Lots of <code>kubectl</code> commands are direct equivalent of their <code>docker</code> / <code>podman</code> counterpart.</p>
<ul>
<li>Show logs for one of your running Pod
<ul>
<li>Equivalent of <code>docker logs</code></li>
<li>You can also target a Deployment or Service</li>
</ul>
</li>
<li>Run a <code>sh</code> session in a Pod's container
<ul>
<li>Equivalent of <code>docker exec -it [container] sh</code></li>
<li>From within Pod, try to reach a Pod via it's Service DNS record (eg. <code>curl &lt;service&gt;.&lt;namespace&gt;.svc.cluster.local</code>) specyfing the proper port.</li>
</ul>
</li>
<li>Get Deployment as YAML
<ul>
<li>Use an option of <code>kubectl get</code></li>
</ul>
</li>
<li>Describe a Deployment
<ul>
<li>Equivalent of <code>docker inspect</code></li>
</ul>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="deployments"><a class="header" href="#deployments">Deployments</a></h1>
<h2 id="update-replica-count"><a class="header" href="#update-replica-count">Update replica count</a></h2>
<ul>
<li>List Deployments with <code>kubectl</code>. Identify the ReplicaSet associated to Vote Deployment. </li>
<li>Update Vote deployment to 3 replicas. Observe ReplicaSet state.
<ul>
<li>You can <code>apply</code> directly or use <code>kubectl edit</code></li>
</ul>
</li>
<li>Update Vote deployment back to 1 replica. Observe ReplicaSet state.</li>
<li>Update image in Vote Deployment to use non-existing tag instead. Observe ReplicaSet state.</li>
</ul>
<h2 id="deployment-and-pods"><a class="header" href="#deployment-and-pods">Deployment and Pods</a></h2>
<p>How does a Deployment identify which Pods it must manage?</p>
<h2 id="deployment-rollout-and-rollback"><a class="header" href="#deployment-rollout-and-rollback">Deployment rollout and rollback</a></h2>
<p>Update Vote deployment image to a non-existing image (make it fail on purpose).</p>
<ul>
<li>Use <code>kubectl rollout status</code> to observe deployment in progress.</li>
<li>Observe ReplicaSet state. </li>
<li>Use another command of <code>kubectl rollout</code> to rollback buggy deployment.</li>
</ul>
<h2 id="strategy"><a class="header" href="#strategy">Strategy</a></h2>
<p>By default, a Deployment update strategy is RollingUpdate. </p>
<ul>
<li>Find other Deployment strategies</li>
<li>Update Vote deployment to use this strategy and test it</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="jobs-and-cronjobs"><a class="header" href="#jobs-and-cronjobs">Jobs and CronJobs</a></h1>
<p>Use a CronJob to vote every minute (yeah, that's cheating)</p>
<h2 id="deploy-cronjob"><a class="header" href="#deploy-cronjob">Deploy CronJob</a></h2>
<ul>
<li>Deploy CronJob <code>resources/cronjob.yml</code></li>
<li>Analyze CronJob template content. Why is <code>spec</code> specified multiple times?</li>
<li>What's the CronJob schedule ?</li>
</ul>
<h2 id="parallelism"><a class="header" href="#parallelism">Parallelism</a></h2>
<p>Update your CronJob to run 3 instances of Jobs every minute instead of 1. </p>
<h2 id="trigger-job-manually-on-demand"><a class="header" href="#trigger-job-manually-on-demand">Trigger Job manually on-demand</a></h2>
<p>Run a <code>kubectl</code> command to trigger manually Job from CronJob without waiting for schedule time.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="daemonsets"><a class="header" href="#daemonsets">DaemonSets</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="statefulsets"><a class="header" href="#statefulsets">StatefulSets</a></h1>
<p>Deploy a Postgres database as StatefulSet by running command:</p>
<pre><code>helm install my-postgres oci://registry-1.docker.io/bitnamicharts/postgresql -f resources/postgres-sts-values.yml
</code></pre>
<p>This command created a a few StatefulSets. <em>Note: Helm is a package manager for Kubernetes, an equivalent of <code>apt</code> or <code>yum</code> for Linux. We'll come back to Helm later.</em></p>
<ul>
<li>Use <code>kubectl</code> to describe Pods and PersistentVolumes. Observe the naming of pods.</li>
<li>Try to delete a Pod and observe result.</li>
<li>Delete Postgres Helm chart release with <code>helm delete my-postgres</code>. What happened to volumes?</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="nodeport"><a class="header" href="#nodeport">NodePort</a></h1>
<p>Example Voting App has a few services to access each component.</p>
<ul>
<li>Update <code>vote</code> and <code>result</code> service to use <code>NodePort</code> service type such as:
<pre><code class="language-yaml">spec:
  type: NodePort
  ports:
  - name: &quot;result-service&quot;
      port: 5001
      targetPort: 80
      # Mind port conflict !
      # Since we're all on the same cluster, 
      # everyone needs to use different ports. 
      # Use a port between 31000 and 36000
      nodePort: 31001
</code></pre>
</li>
<li>Access Vote and Result via a web browser using a Node's public IP address or hostname.
<ul>
<li>Even if Node port is open and listening, we also need a network and firewall rules to accept incoming traffic </li>
</ul>
</li>
<li>What are <code>port</code>, <code>targetPort</code> and <code>nodePort</code> referring to?</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="loadbalancer"><a class="header" href="#loadbalancer">LoadBalancer</a></h1>
<p>A LoadBalancer service will provision automatically a Cloud Load Balancer pointing to our Nodes.</p>
<ul>
<li>Update Vote service to make it of type <code>LoadBalancer</code>
<ul>
<li>You can remove <code>nodePort: xxx</code> from Service definition</li>
</ul>
</li>
<li>Observe change in service behavior
<ul>
<li>Use <code>kubectl describe|get -o yaml</code> to observe new status</li>
</ul>
</li>
<li>Change back type to <code>ClusterIP</code> and observe Cloud Load Balancer deletion
<ul>
<li>Make sure to do that because Load Balancers cost $$$, thanks :)</li>
</ul>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="ingresses"><a class="header" href="#ingresses">Ingresses</a></h1>
<p>Deploy Ingress with Vote configuration:</p>
<pre><code>kubectl apply -f resources/ingress.yml
</code></pre>
<ul>
<li>How does the Ingress map domain name <code>vote.&lt;YOUR_NAME&gt;.k8s.crafteo.io</code> to Vote Service?</li>
<li>Add a similar configuration for <code>result.&lt;YOUR_NAME&gt;.k8s.crafteo.io</code> and Result Service</li>
</ul>
<h2 id="ingress-controller"><a class="header" href="#ingress-controller">Ingress Controller</a></h2>
<p>An Ingress does not work by its own - it needs an Ingress Controller. </p>
<p>Traefik is currently deployed and act as Ingress Controller:</p>
<ul>
<li>Identify the LoadBalancer service used by Traefik</li>
<li>Is it possible to deploy multiple Ingress Controllers ? </li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="externalname-services"><a class="header" href="#externalname-services">ExternalName Services</a></h1>
<p>Create an ExternalName service pointing to google.com:</p>
<pre><code>kubectl apply -f resources/externalname-service.yml 
</code></pre>
<ul>
<li>Run a shell in the Vote container with <code>kubectl exec</code></li>
<li>Try to <code>curl https://external-test</code> and observe result</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="advanced-services-concepts"><a class="header" href="#advanced-services-concepts">Advanced Services concepts</a></h1>
<h2 id="endpoints-and-endpointslices"><a class="header" href="#endpoints-and-endpointslices">Endpoints and EndpointSlices</a></h2>
<p>Services uses EndpointSlices (and Endpoints) under the hood to redirect traffic.</p>
<ul>
<li>Scale Vote deployment to 3 replicas</li>
<li>Identify EndpointSlices and Endpoints created for Vote service</li>
</ul>
<p>Historically, Endpoints were used to link Services to Pods. EndpointSlices provide a more complete API and will eventually replace Endpoints. See <a href="https://kubernetes.io/docs/concepts/services-networking/endpoint-slices/#motivation">Kubernetes doc for details</a></p>
<h2 id="headless-services"><a class="header" href="#headless-services">Headless Services</a></h2>
<p>Headless services can be used when load balancing is not required for Services. Headless services won't have any IP.</p>
<p>Using selectors, internal DNS query will directly return all IPs from existing pods.</p>
<ul>
<li>Scale Vote and Service deployments to 3 replicas</li>
<li>Delete Vote service</li>
<li>Update Vote service template with <code>spec.clusterIP: &quot;None&quot;</code></li>
<li>Re-deploy Vote service</li>
<li>Execute a shell in Vote Pod and observe DNS record behavior between <code>vote</code> and <code>result</code> service</li>
</ul>
<pre><code># Install DNS utils on the fly for testing
apt update &amp;&amp; apt install dns-utils

nslookup vote
nslookup result
</code></pre>
<h2 id="service-discovery"><a class="header" href="#service-discovery">Service discovery</a></h2>
<p>Services are exposed to Pods via environment variables. </p>
<ul>
<li>Execute a shell in Vote pod and explore available services
<ul>
<li>Use <code>env</code> to show all environment variables, and <code>env | grep</code> to filter</li>
</ul>
</li>
<li>Find Result service environment variables </li>
</ul>
<p>A more common approach is to use DNS records pointing to Services.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="configmap"><a class="header" href="#configmap">ConfigMap</a></h1>
<p>ConfigMap are used to mount configurations or environment variables in Pods and containers.</p>
<h1 id="environment-variables"><a class="header" href="#environment-variables">Environment variables</a></h1>
<p>Database Deployment define environment variables for Postgres user and password. Replace them with those defined in ConfigMap <code>resources/config/configmap-postgres-env.yml</code></p>
<ul>
<li>Create ConfigMap with <code>kubectl apply</code></li>
<li>Update Deployment to use ConfigMap to load environment variables.  Use something like this in Pod template:</li>
</ul>
<pre><code class="language-yaml">    envFrom:
    - configMapRef:
        name: db-env
</code></pre>
<h1 id="files-as-volumes"><a class="header" href="#files-as-volumes">Files as Volumes</a></h1>
<p>Use ConfigMap <code>resources/config/configmap-postgres-config.yml</code> to mount a custom Postgres configuration in container at <code>/etc/postgresql/postgresql.conf</code></p>
<ul>
<li>Equivalent of <code>docker run -v &quot;$PWD/my-postgres.conf&quot;:/etc/postgresql/postgresql.conf</code></li>
</ul>
<p>Use something lik this in Pod spec:</p>
<pre><code class="language-yaml">    spec:
      # [...]
      containers:
      - name: postgres
        # [...]
        # Use custom config file
        args: [ &quot;-c&quot;, &quot;config_file=/etc/postgresql/postgresql.conf&quot;]
        # Mount custom config in container
        volumeMounts:
        - mountPath: /etc/postgresql
          name: config-vol
      volumes:
      - name: config-vol
        configMap:
          name: db-config
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="secret"><a class="header" href="#secret">Secret</a></h1>
<p>Secrets are much like ConfigMaps, except they are (theoretically) encrypted in Control Plane. Their access / usage should be protected with proper authorization in cluster.</p>
<p>Secrets require their config to be Base64 encoded. Use a command to encode a string in Base64:</p>
<pre><code>echo -n &quot;mypassword&quot; | base64
</code></pre>
<p><em>Note: Base64 is NOT encryption. It should not be used to protect a secret. Base64 is used as secrets can contain binary data and their base64 representation can be used as plain strings.</em></p>
<h1 id="environment-variables-1"><a class="header" href="#environment-variables-1">Environment variables</a></h1>
<p>Use Secret <code>resources/config/secret-postgres-env.yml</code> to set environment variables in Deployment</p>
<h1 id="files-as-volumes-1"><a class="header" href="#files-as-volumes-1">Files as Volumes</a></h1>
<p>Use Secret <code>resources/config/secret-postgres-config.yml</code> to mount a custom Postgres configuration in container at <code>/etc/postgresql/postgresql.conf</code></p>
<h1 id="use-plain-string-in-secrets"><a class="header" href="#use-plain-string-in-secrets">Use plain string in secrets?</a></h1>
<p>Find a way to use plain strings instead of base64 data in your Secret YAML manifest.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="persistent-volume-claims"><a class="header" href="#persistent-volume-claims">Persistent Volume Claims</a></h1>
<p>Persistent Volume Claims (PVC) is a request for storage. By creating a PVC, you inform the cluster that you need a storage space and cluster will try to fulfill it.</p>
<ul>
<li>Create a PVC from <code>resources/volumes/pvc.yml</code></li>
<li>Update Database deployment to attach the PVC you created. Use something like:</li>
</ul>
<pre><code class="language-yml">apiVersion: apps/v1
kind: Deployment
# [...]
spec:
  template:
    spec:
      containers:
      - name: postgres
        # [...]
        env:
        # postgres requires directory where data are created to be empty
        # Using default /var/lib/postgresql/data would result in failure
        # as it contains a &quot;lost.found&quot; directory, preventing server to initialize
        # Use this trick to have data in a sub-directory of created volume
        - name: PGDATA
          value: &quot;/var/lib/postgresql/data/pg&quot;
          
        volumeMounts:
        - mountPath: /var/lib/postgresql/data
          name: db-data
      volumes:
      - name: db-data
        persistentVolumeClaim:
          claimName: db-data-pvc # Name must match PVC name
</code></pre>
<ul>
<li>Find the Persistent Volume (PV) created as per your request for storage via PVC </li>
<li>Destroy and recreate Database deployment, verify PVC remained intact</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="emptydir-volume"><a class="header" href="#emptydir-volume">emptyDir volume</a></h1>
<p><code>emptyDir</code> can be used in various situation, such as sharing data between container of the same Pod. For example to create a backup of database with a <code>postgres</code> container and upload result to AWS S3 with <code>aws-cli</code> container:</p>
<p>A backup CronJob is present at <code>resources/volumes/cronjob.yml</code> but lacks Volume configs. Adapt template to:</p>
<ul>
<li>Declare an <code>emptyDir</code> volume (use <code>volumes:</code> in appropriate place)</li>
<li>Mount the volume at <code>/backup</code> for both containers </li>
<li>Create the CronJob and trigger it (create a Job from the CronJob) with
<pre><code class="language-sh">kubectl create job --from=cronjob/postgres-backup manual-backup
</code></pre>
</li>
<li>Observe the result </li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="helm-package-manager-for-kubernetes"><a class="header" href="#helm-package-manager-for-kubernetes">Helm: package manager for Kubernetes</a></h1>
<p><a href="https://helm.sh/">Helm</a> is a &quot;package manager&quot; for Kubernetes. It uses YAML templates to manage Kubernetes resources and can be shared other package format (Container, NPM, etc.)</p>
<p>You can find Helm charts on <a href="https://helm.sh/">Artifact Hub</a> or any search engine.</p>
<p>Usage example:</p>
<pre><code class="language-sh">helm --help

# Before installing a chart, adding a Repository is often required
# Adding &quot;bitnami&quot; repository
helm repo add bitnami https://charts.bitnami.com/bitnami

# You can also search for a Helm chart
# Search for &quot;redis&quot; charts
helm search repo redis

# Install a Redis chart
# 'myredis' is the Release name (like a container name for an image)
# 'bitnami/redis' is the chart to install
helm install myredis bitnami/redis
</code></pre>
<p>Adding a repository is not always required, a recent update allow to specify directly an OCI registry such as:</p>
<pre><code class="language-sh">helm install my-redis oci://registry-1.docker.io/bitnamicharts/redis
</code></pre>
<h2 id="install-a-wordpress-chart"><a class="header" href="#install-a-wordpress-chart">Install a Wordpress chart</a></h2>
<p>Find a Wordpress chart and install it. Try to reach it externally using a port-forward.</p>
<ul>
<li>Wordpress is a blogging system with a database. It provides a simple frontend we'll try to reach from the internet.</li>
<li>Look for a Wordpress chart using any method</li>
</ul>
<p>Once done, uninstall Wordpress your Wordpress chart's release</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="helm-advanced-writing-a-helm-charts"><a class="header" href="#helm-advanced-writing-a-helm-charts">Helm advanced: writing a Helm charts</a></h1>
<p>Directory <code>resources/helm/example-voting-app</code> contains a Helm chart with templated YAML manifests.</p>
<h2 id="deployment-update-and-values"><a class="header" href="#deployment-update-and-values">Deployment, update and values</a></h2>
<p>Use <code>helm install</code> command to deploy a release of the Example Voting App chart.</p>
<ul>
<li>Verify it works via port-forward</li>
<li>Explore YAML templates to understand templating mechanisms and its connection to <code>values.yml</code></li>
</ul>
<p>You can override <code>values.yml</code> with external configuration files, typically by environment.</p>
<ul>
<li>Update your release to override the default values using <code>resources/helm/values/dev.yml</code></li>
</ul>
<h2 id="secrets-management"><a class="header" href="#secrets-management">Secrets Management</a></h2>
<p>Helm doesn't provide a native mechanism for managing secrets. A common pattern is to reference an external secret (not managed by the chart).</p>
<p>Update your Helm release using <code>resources/helm/values/prod.yml</code> values.</p>
<ul>
<li>This configuration references an external secret, <strong>you must create it on your own</strong></li>
<li>Explore the contents of the chart to understand the underlying mechanism</li>
</ul>
<p>There are also <a href="https://helm.sh/docs/topics/plugins/">Helm plugins</a> that allow secret management such as <a href="https://github.com/jkroepke/helm-secrets"><code>helm-secrets</code></a></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="kustomize"><a class="header" href="#kustomize">Kustomize</a></h1>
<p>Kustomize is a built-in <code>kubectl</code> tool for multi-environment and configuration management features. </p>
<p>See <a href="https://kubectl.docs.kubernetes.io">Kustomize Official documentation</a> and <a href="https://kubectl.docs.kubernetes.io/references/kustomize/">reference documentation</a></p>
<h2 id="kustomize-example-voting-app"><a class="header" href="#kustomize-example-voting-app">Kustomize Example Voting App</a></h2>
<p>Kustomized applications need:</p>
<ul>
<li>A <em>base</em> with our YAML config (Deployment, Services, etc.) and a <code>kustomization.yml</code> listing desired resources along with some options.</li>
<li>One or more overrides (typically per environment, but not necessarily)</li>
</ul>
<p>To Kustomize Example Voting App:</p>
<ul>
<li>Copy <code>resources/kustomize/base-kustomization.yml</code> into <code>base/kustomization.yml</code>. This file references all Example Voting App resources.</li>
<li>Use one of <code>resources/kustomize/dev</code> or <code>resources/kustomize/prod</code> with <code>kubectl</code>. Note that they each reference their base using a relative path, eg. <code>resources: [ &quot;../../../base&quot; ]</code></li>
</ul>
<pre><code class="language-sh">#
# Mind the -k
#
# Like kubectl apply -f but will read from dev and all referenced resources (including base)
kubectl apply -n &lt;YOU&gt; -k resources/kustomize/dev
</code></pre>
<p>Observe result based on dev and base <code>kustomization.yml</code> files content, especially <code>commonLabels</code></p>
<h2 id="set-namespace"><a class="header" href="#set-namespace">Set namespace</a></h2>
<p>Create a namespace using your name prefixed with <code>-kustomize</code>, eg. <code>YOU-kustomize</code>.</p>
<p>Update dev's <code>kustomization.yml</code> to set namespace <code>YOU-kustomize</code> and apply without <code>-n xxx</code> flag, eg:</p>
<pre><code class="language-sh">kubectl apply -k resources/kustomize/dev
</code></pre>
<p>What happened to resources in previous namespace ?</p>
<h2 id="patches-and-configmapsecret-generators"><a class="header" href="#patches-and-configmapsecret-generators">Patches and ConfigMap/Secret generators</a></h2>
<p>Use a <a href="https://kubectl.docs.kubernetes.io/guides/config_management/secrets_configmaps/">ConfigMap generator</a> to create a ConfigMap from file <code>postgresql.conf</code> file</p>
<p>Use a <a href="https://kubectl.docs.kubernetes.io/guides/config_management/secrets_configmaps/">Secret generator</a> to create a Secret from file <code>postgres-secret.properties</code> file</p>
<ul>
<li>You'll need additional options to consider this file as environment variables</li>
</ul>
<p>Override Deployment using a <em>patch</em> to ensure Secret and ConfigMap values are used instead of values set in base.</p>
<ul>
<li>Use <code>db-deployment-patch.yml</code> and update <code>kustomization.yml</code></li>
</ul>
<h2 id="adapt-another-kustomization"><a class="header" href="#adapt-another-kustomization">Adapt another Kustomization</a></h2>
<p>Reproduce changes from <code>dev</code> Kustomization to <code>prod</code> Kustomization and deploy them both in the same namespace. </p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="ingress-with-tls-https"><a class="header" href="#ingress-with-tls-https">Ingress with TLS (HTTPS)</a></h1>
<p><code>resources/https</code> contains an Ingress resource configured with TLS (HTTPS) and a <a href="https://github.com/cert-manager/cert-manager">Cert Manager</a> Certificate.</p>
<p>Deploy this Ingress and Certificate along with Example Voting App</p>
<ul>
<li>Make sure Example Voting App is deployed</li>
<li>Deploy resources in <code>resources/https</code>. **You must replace <code>&lt;YOUR_NAME&gt;</code> in YAML files by your own name. **</li>
<li>Observe creation of Ingress and Certificate resources</li>
<li>Test functionnality. The Ingress should be reachable externally. </li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="pod-resources--requests"><a class="header" href="#pod-resources--requests">Pod Resources / Requests</a></h1>
<p>Pods can have <a href="https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/">resources requests and limits</a> such as:</p>
<pre><code class="language-yaml">spec:
  template:
    spec:
      containers:
      - name: # ...
        resources:
          requests:
            cpu: 1
            memory: &quot;1Gi&quot;
          limits:
            cpu: 2
            memory: 2Gi
</code></pre>
<p>Affect resources to your Pods:</p>
<ul>
<li>Update Vote Pods to request 1 CPU and 1024Mi memory and apply</li>
<li>Update Vote Pods to request 0.2 CPU and 128Mi memory and apply</li>
<li>What units can you use to specify resources and requests ? What's the difference between <code>128M</code> and <code>128Mi</code> ?</li>
</ul>
<p>Requests and limits affect scheduling differently. Set Vote's Pods resources as below and scale to 10 replicas. Observe result.</p>
<pre><code class="language-yml">resources:
  requests:
    cpu: 1m
    memory: 2Mi
  limits:
    cpu: 1
    memory: 1Gi
</code></pre>
<p>Now set resources as below and scale to 10 replicas and observe result.</p>
<pre><code class="language-yml">resources:
  requests:
    cpu: 250m
    memory: 256Mi
  limits:
    cpu: 1
    memory: 1Gi
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="horizontal-scalability-and-horizontal-pod-autoscaler-hpa"><a class="header" href="#horizontal-scalability-and-horizontal-pod-autoscaler-hpa">Horizontal Scalability and Horizontal Pod Autoscaler (HPA)</a></h1>
<p>Let's configure an Horizontal Pod Autoscaler (HPA) to automatically scale our Vote Pods when their CPU load is high. We'll use a Auto Voter Pod to emulate a CPU charge (lots of vote !)</p>
<p>Update Vote deployment to specify resources and requests such as:</p>
<pre><code class="language-yaml">        resources:
          requests:
            cpu: 50m
            memory: 128Mi
          limits:
            cpu: 100m
            memory: 256Mi
</code></pre>
<p>Deploy Auto Voter: <code>kubectl apply -f resources/scaling/auto-voter.yml</code></p>
<ul>
<li>It's a simple bash loop sending POST request to Vote service.</li>
<li>Observe Vote CPU load with <code>kubectl top</code> command (CPU load is ~2m idle, and should now be around ~50m). For example:</li>
</ul>
<pre><code class="language-sh">watch kubectl top pod vote-xxx
</code></pre>
<p>Deploy HPA in <code>resources/scaling/hpa.yml</code> and observe scalability</p>
<ul>
<li>How does HPA know when a pod should be scaled-up?</li>
</ul>
<p>Delete Auto Voter and observe HPA scale down. </p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="vertical-scalability-and-cluster-autoscaler"><a class="header" href="#vertical-scalability-and-cluster-autoscaler">Vertical Scalability and Cluster Autoscaler</a></h1>
<p>The Cluster Autoscaler is responsible to continuously observe cluster state and add or remove nodes as needed. It uses resources requests and Pod scheduling capacity to identify when to add or remove nodes.</p>
<p>Configure Vote deployment to have resources such as:</p>
<pre><code class="language-yaml">        resources:
          requests:
            cpu: 500m
            memory: 512Mi
          limits:
            cpu: 1000m
            memory: 1024Mi
</code></pre>
<p>Then scale Vote deployment to have 20 Pods</p>
<pre><code class="language-sh">kubectl scale deployment vote --replicas 20
</code></pre>
<p>Observe Pod behavior:</p>
<ul>
<li>Pods are scheduled and remain Pending</li>
<li>New nodes are added to make more room for new Pods (up to a certain maximum)</li>
</ul>
<p>Then scale back your Deployment to 1</p>
<ul>
<li>Observe Nodes are removed</li>
</ul>
<p>Identify the Cluster Autoscaler in <code>kube-system</code> responsible for autoscaling. </p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="liveness-readiness-and-startup-probes"><a class="header" href="#liveness-readiness-and-startup-probes">Liveness, Readiness and Startup Probes</a></h1>
<p><a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/">Liveness, Readiness and Startup Probes</a> define behavior to improve health of Pods in various situations.</p>
<h2 id="liveness-probe"><a class="header" href="#liveness-probe">Liveness probe</a></h2>
<p>Liveness probe check health of container regularly, restarting it if it fails. Note that the <em>container</em> itself is restarted (killed and recreated), not the Pod. </p>
<p>Add a Liveness Probe on Vote container such as:</p>
<pre><code class="language-yml">        livenessProbe:
         httpGet:
           path: /
           port: 80
         periodSeconds: 2 # This is normally around ~10s, we use shorter time to observe behavior
</code></pre>
<p>Apply your changes. This should not have any impact on Pod since this healthcheck works.</p>
<p>To test Liveness behavior, override Vote container <code>command</code> such as:</p>
<pre><code class="language-yaml">        command: [&quot;sleep&quot;, &quot;infinity&quot;]
</code></pre>
<p>This will cause Liveness probe to fail since, instead of starting Vote server, container will run a sleep command. Apply and observe behavior (use <code>kubectl describe</code> to show events)</p>
<p>Update liveness probe to start 10s after container initial start with a failure theshold of 8.</p>
<h2 id="readiness-probe"><a class="header" href="#readiness-probe">Readiness probe</a></h2>
<p>Add a Readiness Probe on Vote container (keep the Liveness Probe). Use a similar method as before to test behavior.</p>
<p>Describe Vote service and verify that no traffic is routed to non-Ready Pod. </p>
<h2 id="startup-probe"><a class="header" href="#startup-probe">Startup probe</a></h2>
<p>Add a Startup Probe on Vote container (keep Liveness and Readiness Probes). Use a similar method as before to test behavior.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="pod-disruption-budget-pdb"><a class="header" href="#pod-disruption-budget-pdb">Pod Disruption Budget (PDB)</a></h1>
<p>Pod Disruption Budget (PDB) are used to specify how much Pod can become unavailable during an update. See <a href="https://kubernetes.io/docs/tasks/run-application/configure-pdb/">official doc</a></p>
<p>Update Vote Deployment to have 5 replicas, a Readiness Probe and prefer Pods to be deployed on a single node, eg:</p>
<pre><code class="language-yml">apiVersion: apps/v1
kind: Deployment
# ...
spec:
  replicas: 5
  template:
    spec:
      affinity:
        nodeAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - weight: 100
              preference:
                matchExpressions:
                  - key: &quot;kubernetes.io/hostname&quot;
                    operator: In
                    values:
                      - &quot;ip-192-168-1-4.eu-west-3.compute.internal&quot;
      containers:
      - name: vote
        # ...
        readinessProbe:
          httpGet:
            path: /
            port: 80
          initialDelaySeconds: 5
</code></pre>
<p>Create a PDB for Vote to allow max 1 Pod unavailable such as:</p>
<pre><code class="language-yml">apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: vote-pdb
spec:
  minAvailable: 4
  selector:
    matchLabels:
      app: vote
</code></pre>
<p>Then drain the Node on which you deployed your Pods to observe behavior.</p>
<pre><code class="language-sh"># Example command to drain a node
# Replaced with your node name
kubectl drain ip-192-168-1-160.eu-west-3.compute.internal --delete-emptydir-data=true --ignore-daemonsets=true
</code></pre>
<p>Reproduce with <code>maxUnavailable: 2</code> instead of <code>minAvailable: 1</code></p>
<p>What happens when you set desired value as percentage and Pod number is not round? (eg. minAvailable of 50% out of 5 replicas)</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="pod-scheduling-introduction"><a class="header" href="#pod-scheduling-introduction">Pod scheduling: introduction</a></h1>
<p>Pod scheduling allow to specify Nodes constraints on which Pod will be scheduled. See <a href="https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity">official documentation</a></p>
<h2 id="node-selector-affect-pods-directly-to-nodes"><a class="header" href="#node-selector-affect-pods-directly-to-nodes">Node selector: affect Pods directly to Node(s)</a></h2>
<p>Use <code>nodeSelector</code> to assign Vote Deployment's Pods to a given Node using the <code>kubernetes.io/hostname</code> label.</p>
<ul>
<li>Use <code>kubectl describe node</code> to identify suitable Node labels</li>
<li>Update Vote Deployment to defne <code>nodeSelector</code> and apply</li>
</ul>
<h2 id="affinity-and-anti-affinity"><a class="header" href="#affinity-and-anti-affinity">Affinity and anti-affinity</a></h2>
<p>Affinity and Anti-affinity comes in two flavors, as per doc:</p>
<blockquote>
<p><code>requiredDuringSchedulingIgnoredDuringExecution</code>: The scheduler can't schedule the Pod unless the rule is met. This functions like nodeSelector, but with a more expressive syntax.
<code>preferredDuringSchedulingIgnoredDuringExecution</code>: The scheduler tries to find a node that meets the rule. If a matching node is not available, the scheduler still schedules the Pod.</p>
</blockquote>
<p>In short:</p>
<ul>
<li><code>requiredDuringSchedulingIgnoredDuringExecution</code> is a <strong>hard</strong> constraint: Pod won't be scheduled unless constraint is met</li>
<li><code>preferredDuringSchedulingIgnoredDuringExecution</code> is a <strong>soft</strong> constraint: Pod is prefered to meet criterias, but will be scheduled nonetheless if not met.</li>
</ul>
<p>Both <code>required</code> and <code>preferred</code> can be set at the same time for complex behavior.</p>
<p>For example, this define a Pod affinity requiring a Pod to be schedule on a specific Node, reproducing a behavior similar to Node Selector</p>
<pre><code class="language-yml">spec:
  template:
    spec:
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
             - matchExpressions:
                 - key: &quot;kubernetes.io/hostname&quot;
                   operator: In
                   values:
                     - &quot;ip-192-168-1-189.eu-west-3.compute.internal&quot;
</code></pre>
<p>Update Vote Deployment to:</p>
<ul>
<li>Require Pods to be scheduled in Zone <code>eu-west-3b</code> (use label <code>topology.kubernetes.io/zone: eu-west-3b</code>)</li>
<li>Prefer Pods to be scheduled on a specific <code>kubernetes.io/hostname</code></li>
<li>If your Deployment or Pods are stuck fo some reason, delete Deployment and and re-create it</li>
</ul>
<p>Apply changes and observe Pod scheduling behavior. Destroy all Vote's Pods and observe their scheduling. </p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="inter-pod-affinity-and-anti-affinity"><a class="header" href="#inter-pod-affinity-and-anti-affinity">Inter-pod Affinity and Anti-affinity</a></h1>
<blockquote>
<p>Inter-pod affinity and anti-affinity allow you to constrain which nodes your Pods can be scheduled on based on the labels of Pods already running on that node, instead of the node labels.</p>
</blockquote>
<p>Source: <a href="https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity">official doc</a></p>
<h2 id="spread-pods-on-nodes"><a class="header" href="#spread-pods-on-nodes">Spread Pods on Nodes</a></h2>
<p>Context: you want to spread Vote Pods on your nodes to even load and improve redundancy and resilience. </p>
<p>Prefer scheduling Vote Pods on Node which doesn't already have one using an affinity such as:</p>
<pre><code class="language-yml">      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - topologyKey: &quot;kubernetes.io/hostname&quot;
            labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - vote
            namespaceSelector:
              matchExpressions:
              - key: kubernetes.io/metadata.name
                operator: In
                values:
                  - &lt;YOUR NAME&gt;
</code></pre>
<h2 id="deploy-pods-together"><a class="header" href="#deploy-pods-together">Deploy Pods together</a></h2>
<p>Context: for better performance, you want to have Redis Pods as close as possible to Vote Pods by preferring spreading Redis Pods on Nodes and preferring Vote Pods to deploy on a Node already running a Redis Pod (so that cache can be reached on the same Node, reducing network latency). </p>
<p>Inter-pod anti-affinity:</p>
<ul>
<li>Configure Redis Deployment to <strong>prefer</strong> scheduling on Nodes without a Redis Pod already running.</li>
</ul>
<p>Inter-pod affinity:</p>
<ul>
<li>Configure Vote Deployment to <strong>require</strong> scheduling on a Node which already have a Redis Pod (for communication optimization) and <strong>prefer</strong> a Node which doesn't already have a Vote Pod. </li>
</ul>
<p>Play with replicas on Redis and Vote deployment to observe results.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="topology-spread-constraint"><a class="header" href="#topology-spread-constraint">Topology Spread Constraint</a></h1>
<p><a href="https://kubernetes.io/docs/concepts/scheduling-eviction/topology-spread-constraints/">Topology Spread Constraints</a> allow you to spread workload across domains. It's useful to better achieve HA while being easier to use than Affinity. </p>
<p><code>topologySpreadConstraints</code> uses <code>topologyKey</code> to spread Pods around the domains matching given key, for example:</p>
<pre><code class="language-yaml">    topologySpreadConstraints:
      - maxSkew: 1
        topologyKey: topology.kubernetes.io/zone
        whenUnsatisfiable: DoNotSchedule
        labelSelector:
          matchLabels:
            app: vote
</code></pre>
<p>Will spread Vote Pods across the Zone Domain. </p>
<p>Update Vote Deployment to even loads in current region. </p>
<p>What's the behavior of <code>topologySpreadConstraints</code> in relation to Node/Pod Affinity ? </p>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->


                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">

            </nav>

        </div>




        <script>
            window.playground_copyable = true;
        </script>


        <script src="elasticlunr.min.js"></script>
        <script src="mark.min.js"></script>
        <script src="searcher.js"></script>

        <script src="clipboard.min.js"></script>
        <script src="highlight.js"></script>
        <script src="book.js"></script>

        <!-- Custom JS scripts -->

        <script>
        window.addEventListener('load', function() {
            window.setTimeout(window.print, 100);
        });
        </script>

    </div>
    </body>
</html>
