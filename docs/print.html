<!DOCTYPE HTML>
<html lang="en" class="light" dir="ltr">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>Crafteo - Kubernetes Training</title>
        <meta name="robots" content="noindex">


        <!-- Custom HTML head -->
        
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff">

        <link rel="icon" href="favicon.svg">
        <link rel="shortcut icon" href="favicon.png">
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        <link rel="stylesheet" href="css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="highlight.css">
        <link rel="stylesheet" href="tomorrow-night.css">
        <link rel="stylesheet" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->

    </head>
    <body class="sidebar-visible no-js">
    <div id="body-container">
        <!-- Provide site root to javascript -->
        <script>
            var path_to_root = "";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('light')
            html.classList.add(theme);
            var body = document.querySelector('body');
            body.classList.remove('no-js')
            body.classList.add('js');
        </script>

        <input type="checkbox" id="sidebar-toggle-anchor" class="hidden">

        <!-- Hide / unhide sidebar before it is displayed -->
        <script>
            var body = document.querySelector('body');
            var sidebar = null;
            var sidebar_toggle = document.getElementById("sidebar-toggle-anchor");
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            } else {
                sidebar = 'hidden';
            }
            sidebar_toggle.checked = sidebar === 'visible';
            body.classList.remove('sidebar-visible');
            body.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded affix "><a href="welcome.html">Crafteo - Formation Kubernetes</a></li><li class="chapter-item expanded affix "><li class="part-title">Introduction</li><li class="chapter-item expanded "><a href="intro/pod.html"><strong aria-hidden="true">1.</strong> Notre premier Pod</a></li><li class="chapter-item expanded "><a href="intro/deployment.html"><strong aria-hidden="true">2.</strong> Deployment</a></li><li class="chapter-item expanded "><a href="intro/service.html"><strong aria-hidden="true">3.</strong> Service</a></li><li class="chapter-item expanded "><a href="intro/logs-exec-others.html"><strong aria-hidden="true">4.</strong> Logs, exec et autres opérations</a></li><li class="chapter-item expanded affix "><li class="part-title">Controllers</li><li class="chapter-item expanded "><a href="controllers/deployments.html"><strong aria-hidden="true">5.</strong> Deployments & ReplicaSets</a></li><li class="chapter-item expanded "><a href="controllers/jobs.html"><strong aria-hidden="true">6.</strong> Jobs & CronJobs</a></li><li class="chapter-item expanded "><a href="controllers/daemonsets.html"><strong aria-hidden="true">7.</strong> DaemonSets</a></li><li class="chapter-item expanded "><a href="controllers/statefulsets.html"><strong aria-hidden="true">8.</strong> StatefulSets</a></li><li class="chapter-item expanded affix "><li class="part-title">Services & Réseau</li><li class="chapter-item expanded "><a href="services/nodeport.html"><strong aria-hidden="true">9.</strong> NodePort</a></li><li class="chapter-item expanded "><a href="services/loadbalancers.html"><strong aria-hidden="true">10.</strong> LoadBalancer</a></li><li class="chapter-item expanded "><a href="services/ingresses.html"><strong aria-hidden="true">11.</strong> Ingresses et Ingress Controllers</a></li><li class="chapter-item expanded "><a href="services/externalname.html"><strong aria-hidden="true">12.</strong> ExternalName Services</a></li><li class="chapter-item expanded "><a href="services/advanced.html"><strong aria-hidden="true">13.</strong> Avancé: Endpoints, Headless Service...</a></li><li class="chapter-item expanded affix "><li class="part-title">Configs & Secrets</li><li class="chapter-item expanded "><a href="config/configmap.html"><strong aria-hidden="true">14.</strong> ConfigMap</a></li><li class="chapter-item expanded "><a href="config/secret.html"><strong aria-hidden="true">15.</strong> Secret</a></li><li class="chapter-item expanded affix "><li class="part-title">Volumes : Persistants & Éphémères</li><li class="chapter-item expanded "><a href="volumes/pvc.html"><strong aria-hidden="true">16.</strong> Volumes persistants: PVC & PV</a></li><li class="chapter-item expanded "><a href="volumes/emptydir.html"><strong aria-hidden="true">17.</strong> Volumes éphémères: emptyDir</a></li><li class="chapter-item expanded affix "><li class="part-title">Outils de déploiement</li><li class="chapter-item expanded "><a href="deploy-tools/helm-intro.html"><strong aria-hidden="true">18.</strong> Helm: introduction</a></li><li class="chapter-item expanded "><a href="deploy-tools/helm-advanced.html"><strong aria-hidden="true">19.</strong> Helm: avancé</a></li><li class="chapter-item expanded "><a href="deploy-tools/kustomize.html"><strong aria-hidden="true">20.</strong> Kustomize</a></li><li class="chapter-item expanded affix "><li class="part-title">Déploiement en production</li><li class="chapter-item expanded "><a href="production/tls.html"><strong aria-hidden="true">21.</strong> Ingress & TLS / HTTPS</a></li><li class="chapter-item expanded "><a href="production/resources.html"><strong aria-hidden="true">22.</strong> Resources Request & Limits</a></li><li class="chapter-item expanded "><a href="production/horizontal-scaling.html"><strong aria-hidden="true">23.</strong> Horizontal Pod Autoscaler</a></li><li class="chapter-item expanded "><a href="production/vertical-scaling.html"><strong aria-hidden="true">24.</strong> Vertical Pod Autoscaler</a></li><li class="chapter-item expanded "><a href="production/probes.html"><strong aria-hidden="true">25.</strong> Liveness, Readiness et Startup Probes</a></li><li class="chapter-item expanded "><a href="production/pdb.html"><strong aria-hidden="true">26.</strong> Pod Disruption Budget (PDB)</a></li><li class="chapter-item expanded affix "><li class="part-title">Scheduling</li><li class="chapter-item expanded "><a href="scheduling/intro.html"><strong aria-hidden="true">27.</strong> Introduction</a></li><li class="chapter-item expanded "><a href="scheduling/topology-spread-constraint.html"><strong aria-hidden="true">28.</strong> Topology Spread Constraint</a></li><li class="chapter-item expanded "><a href="scheduling/node-affinity.html"><strong aria-hidden="true">29.</strong> Node Affinity</a></li><li class="chapter-item expanded "><a href="scheduling/inter-pod-affinity.html"><strong aria-hidden="true">30.</strong> Pod Affinity</a></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle"></div>
        </nav>

        <!-- Track and set sidebar scroll position -->
        <script>
            var sidebarScrollbox = document.querySelector('#sidebar .sidebar-scrollbox');
            sidebarScrollbox.addEventListener('click', function(e) {
                if (e.target.tagName === 'A') {
                    sessionStorage.setItem('sidebar-scroll', sidebarScrollbox.scrollTop);
                }
            }, { passive: true });
            var sidebarScrollTop = sessionStorage.getItem('sidebar-scroll');
            sessionStorage.removeItem('sidebar-scroll');
            if (sidebarScrollTop) {
                // preserve sidebar scroll position when navigating via links within sidebar
                sidebarScrollbox.scrollTop = sidebarScrollTop;
            } else {
                // scroll sidebar to current active section when navigating via "next/previous chapter" buttons
                var activeSection = document.querySelector('#sidebar .active');
                if (activeSection) {
                    activeSection.scrollIntoView({ block: 'center' });
                }
            }
        </script>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky">
                    <div class="left-buttons">
                        <label id="sidebar-toggle" class="icon-button" for="sidebar-toggle-anchor" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </label>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title">Crafteo - Kubernetes Training</h1>

                    <div class="right-buttons">
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>
                        <a href="https://github.com/PierreBeucher/k8s.training.crafteo.io" title="Git repository" aria-label="Git repository">
                            <i id="git-repository-button" class="fa fa-github"></i>
                        </a>

                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script>
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <h1 id="crafteo---formation-kubernetes"><a class="header" href="#crafteo---formation-kubernetes">Crafteo - Formation Kubernetes</a></h1>
<p>Bienvenue sur la page d'exercice Kubernetes ! Utiliser le menu pour accéder aux différents modules.</p>
<p>Plus d'infos sur <a href="https://crafteo.io">crafteo.io</a></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="notre-premier-pod-"><a class="header" href="#notre-premier-pod-">Notre premier Pod !</a></h1>
<p>Les Pods et la plupart des objets Kubernetes sont par crées dans un <em>Namespace</em>. Un Namespace est une sorte d'espace indépendant où notre Pod sera créé dans le cluster Kubernetes pour éviter les conflits avec d'autres objets :</p>
<pre><code class="language-sh">kubectl create namespace &lt;your_ns&gt;
</code></pre>
<p>Ensuite, créez un Pod dans notre namespace (l'option <code>-n</code> spécifie le namespace à utiliser) :</p>
<pre><code class="language-sh">kubectl -n &lt;your_ns&gt; run mypod --image us-docker.pkg.dev/google-samples/containers/gke/whereami:v1.2.21 --port 8080
</code></pre>
<p>C'est l'équivalent de <code>docker run</code> pour Kubernetes.</p>
<p>Pour accéder à votre Pod de l'extérieur, utilisez:</p>
<pre><code class="language-sh">kubectl -n &lt;your_ns&gt; port-forward pod/mypod --address 0.0.0.0 8081:8080
</code></pre>
<ul>
<li><code>port-forward</code> créé un tunnel réseau directement depuis la machine depuis laquelle <code>kubectl</code> est lancé vers le container dans le cluster Kubernetes. Un équivalent de <code>docker run -p 8081:80 [...]</code>. - <code>--address</code> permet de binder l'écoute du port sur toutes les adresses (<code>localhost</code>) est utilisé par defaut pour permettre une connection à distance sur votre machine de TP</li>
</ul>
<p>Votre Pod est maintenant accessible sur le port 8081. Essayez d'y accéder localement ou via un navigateur :</p>
<pre><code class="language-sh">curl YOU.training.crafteo.io:8081
</code></pre>
<hr />
<p>En réalité, <code>kubectl run</code> est rarement utilisé, sauf pour le debug. La plupart du temps, on utilise des <em>manifests ou configurations YAML</em> pour <strong>décrire</strong> les objets Kubernetes.</p>
<p>Créer un Pod en utilisant un manifest YAML avec </p>
<pre><code class="language-sh">kubectl -n &lt;your_ns&gt; apply -f intro/pod.yml
</code></pre>
<p>Maintenant, utiliser les commandes <code>kubectl</code> pour :</p>
<ul>
<li>Lister tous les Pods</li>
<li>Faire un port-forward du pod créé via <code>intro/pod.yml</code></li>
<li>Supprimer les deux Pods avec <code>kubectl delete</code></li>
</ul>
<hr />
<p>Par défaut, <code>kubectl</code> récupère les Pods (et autres objets) dans le namespace <code>default</code>. Vous pouvez changer ce comportement avec :</p>
<pre><code class="language-sh">kubectl config set-context --current --namespace pierre
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="deployment--gérer-plusieurs-pods"><a class="header" href="#deployment--gérer-plusieurs-pods">Deployment : gérer plusieurs Pods</a></h1>
<p>Un Deployment gère un ensemble de Pods.</p>
<ul>
<li>Créer un Deployment avec <code>kubectl</code> à partir du manifest <code>intro/deployment.yml</code>.
<ul>
<li>Utiliser <code>kubectl --help</code>, un moteur de recherche ou une IA si besoin</li>
<li>La commande ressemble à <code>kubectl apply [...]</code></li>
</ul>
</li>
<li>Lister les Deployments avec <code>kubectl get</code></li>
<li>Redémarrer le Pod créé par notre Deployment
<ul>
<li>Il n'y a pas de commande <code>restart</code>, trouver une autre méthode</li>
</ul>
</li>
<li>Mettre à jour votre Deployment pour avoir 3 replicas de notre Pod</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="service-diriger-les-flux-réseaux-vers-nos-pods"><a class="header" href="#service-diriger-les-flux-réseaux-vers-nos-pods">Service: diriger les flux réseaux vers nos Pods</a></h1>
<p>Manager des Services avec des manifests et accéder à un Pod via un Service:</p>
<ul>
<li>Créer un Service avec <code>kubectl</code> en utilisant le manifest <code>intro/service.yml</code>
<ul>
<li>Rappel : <code>kubectl apply -f ...</code></li>
</ul>
</li>
<li>Utiliser <code>kubectl port-forward</code> pour exposer le service localement et tester l'accès
<ul>
<li>Le port forwarding va <em>rediriger</em> un port local vers votre Pod. Utilisez <code>curl localhost:PORT</code> ou équivalent.</li>
<li><code>kubectl port-forward --help</code> ou consulter la documentation officielle</li>
</ul>
</li>
<li>Comment un Service identifie-t-il les Pods à servir ?</li>
</ul>
<hr />
<ul>
<li>Supprimer Pods, Deployment et Service avec une seule commande <code>kubectl</code></li>
<li>Ré-appliquer notre Deployment, Service et Pod avec une seule commande</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="logs-exec-et-autres-opérations"><a class="header" href="#logs-exec-et-autres-opérations"><code>logs</code>, <code>exec</code> et autres opérations</a></h1>
<p>De nombreuses commandes <code>kubectl</code> sont l'équivalent direct de la CLI <code>docker</code> / <code>podman</code> sur Kubernetes.</p>
<ul>
<li>Afficher les logs d'un de vos Pods en cours d'exécution
<ul>
<li>Équivalent de <code>docker logs</code></li>
<li>Vous pouvez aussi cibler un Deployment ou un Service</li>
</ul>
</li>
<li>Lancer un shell <code>sh</code> dans le conteneur d'un Pod
<ul>
<li>Équivalent de <code>docker exec -it [container] sh</code></li>
<li>Depuis votre nouveau shell, essayez d'atteindre un autre Pod via son le nom de domaine associé à son Service (ex : <code>curl &lt;service&gt;.&lt;namespace&gt;.svc.cluster.local</code>) en spécifiant le port approprié.</li>
</ul>
</li>
<li>Afficher un Deployment (ou autre object) au format YAML
<ul>
<li>Utilisez une option de <code>kubectl get</code></li>
</ul>
</li>
<li>Décrire un Deployment
<ul>
<li>Équivalent de <code>docker inspect</code></li>
</ul>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="deployments"><a class="header" href="#deployments">Deployments</a></h1>
<h2 id="modifier-le-nombre-de-replicas"><a class="header" href="#modifier-le-nombre-de-replicas">Modifier le nombre de replicas</a></h2>
<ul>
<li>Lister les Deployments avec <code>kubectl</code>. Identifier le ReplicaSet associé au Deployment Vote.</li>
<li>Mettre à jour le Deployment Vote à 3 replicas. Observer l'état du ReplicaSet.
<ul>
<li>Appliquer directement ou utiliser <code>kubectl edit</code></li>
</ul>
</li>
<li>Repasser le Deployment Vote à 1 replica. Observer l'état du ReplicaSet.</li>
<li>Mettre à jour l'image du Deployment Vote avec un tag inexistant. Observer l'état du ReplicaSet.</li>
</ul>
<h2 id="deployment-et-pods"><a class="header" href="#deployment-et-pods">Deployment et Pods</a></h2>
<p>Comment un Deployment identifie-t-il les Pods qu'il manage ?</p>
<h2 id="rollout-et-rollback-dun-deployment"><a class="header" href="#rollout-et-rollback-dun-deployment">Rollout et rollback d'un Deployment</a></h2>
<p>Mettre à jour l'image du Deployment Vote avec une image inexistante (provoquer un échec volontairement).</p>
<ul>
<li>Utiliser <code>kubectl rollout status</code> pour observer le déploiement en cours.</li>
<li>Observer l'état du ReplicaSet.</li>
<li>Utiliser une autre commande de <code>kubectl rollout</code> pour rollback le déploiement en échec.</li>
</ul>
<h2 id="stratégie"><a class="header" href="#stratégie">Stratégie</a></h2>
<p>Par défaut, la stratégie de mise à jour d'un Deployment est RollingUpdate.</p>
<ul>
<li>Trouver d'autres stratégies de mise à jour pour les Deployments</li>
<li>Mettre à jour le Deployment Vote pour utiliser cette stratégie et la tester</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="jobs-et-cronjobs"><a class="header" href="#jobs-et-cronjobs">Jobs et CronJobs</a></h1>
<p>Utiliser un CronJob pour voter chaque minute (oui, c'est de la triche)</p>
<h2 id="déployer-un-cronjob"><a class="header" href="#déployer-un-cronjob">Déployer un CronJob</a></h2>
<ul>
<li>Déployer le CronJob <code>resources/cronjob.yml</code></li>
<li>Analyser le contenu du template CronJob. Pourquoi <code>spec</code> est-il spécifié plusieurs fois ?</li>
<li>Quand sera executé le CronJob ?</li>
</ul>
<h2 id="parallélisme"><a class="header" href="#parallélisme">Parallélisme</a></h2>
<p>Mettre à jour le CronJob pour lancer 3 instances de Jobs au lieu d'une seule lors du trigger du CronJob.</p>
<h2 id="déclencher-un-job-manuellement-à-la-demande"><a class="header" href="#déclencher-un-job-manuellement-à-la-demande">Déclencher un Job manuellement à la demande</a></h2>
<p>Lancer une commande <code>kubectl</code> pour déclencher manuellement un Job à partir du CronJob sans attendre la prochaine exécution planifiée.</p>
<ul>
<li>Utiliser <code>kubectl create [...]</code></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="daemonsets"><a class="header" href="#daemonsets">DaemonSets</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="statefulsets"><a class="header" href="#statefulsets">StatefulSets</a></h1>
<p>Déployer une base de données Postgres en StatefulSet avec la commande :</p>
<pre><code>helm install my-postgres oci://registry-1.docker.io/bitnamicharts/postgresql -f resources/postgres-sts-values.yml
</code></pre>
<p>Cette commande crée plusieurs StatefulSets. <em>Note : Helm est un package manager pour Kubernetes, l'équivalent de <code>apt</code> ou <code>yum</code> pour Linux. On reviendra sur Helm plus tard.</em></p>
<ul>
<li>Utiliser <code>kubectl</code> pour décrire les Pods et PersistentVolumes. Observer le nommage des pods.</li>
<li>Supprimer un Pod et observer le résultat.</li>
<li>Supprimer le release Helm Postgres avec <code>helm delete my-postgres</code>. Que se passe-t-il pour les volumes ?</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="nodeport"><a class="header" href="#nodeport">NodePort</a></h1>
<p>L'application Example Voting App propose plusieurs services pour accéder à chaque composant.</p>
<ul>
<li>Modifier les services <code>vote</code> et <code>result</code> pour utiliser le type de service <code>NodePort</code>:
<pre><code class="language-yaml">spec:
  type: NodePort
  ports:
  - name: &quot;result-service&quot;
    port: 5001
    targetPort: 80
    # Attention aux conflits de ports !
    # Comme tout le monde est sur le même cluster,
    # chacun doit utiliser des ports différents.
    # Utiliser un port entre 31000 et 36000
    nodePort: 31001
</code></pre>
</li>
<li>Accéder à Vote et Result via un navigateur web en utilisant l'adresse IP publique ou le nom d'hôte d'un Node.
<ul>
<li>Utiliser <code>kubectl get node</code> et <code>kubectl describe node &lt;node-name&gt;</code> pour identifier l'IP publique des Nodes</li>
<li>Même si le port Node est ouvert et à l'écoute, il faut aussi des règles réseau et firewall pour accepter le trafic entrant</li>
</ul>
</li>
<li>À quoi correspondent <code>port</code>, <code>targetPort</code> et <code>nodePort</code> ?</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="loadbalancer"><a class="header" href="#loadbalancer">LoadBalancer</a></h1>
<p>Un service LoadBalancer va automatiquement provisionner un Load Balancer:</p>
<ul>
<li>dans le Provider Cloud correspondant à notre cluster si il est déployé dans le Cloud</li>
<li>Directement via des configurations internes pour un cluster on-prem</li>
</ul>
<p>Configurer un Service de type Load Balancer:</p>
<ul>
<li>Modifier le service Vote pour qu'il soit de type <code>LoadBalancer</code>
<ul>
<li>Optionnel: retirer <code>nodePort: xxx</code> de la définition du Service qui n'est plus nécéssaire</li>
</ul>
</li>
<li>Observer le changement de comportement du service
<ul>
<li>Utiliser <code>kubectl describe|get -o yaml</code> pour observer le nouveau status</li>
</ul>
</li>
<li>Repasser le type à <code>ClusterIP</code> et observer la suppression du Cloud Load Balancer
<strong>- Bien faire cette étape car les Load Balancers coûtent $$$, merci :-)</strong></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="ingresses"><a class="header" href="#ingresses">Ingresses</a></h1>
<p>Déployer un Ingress avec la configuration Vote :</p>
<pre><code>kubectl apply -f resources/ingress.yml
</code></pre>
<ul>
<li>Comment l'Ingress fait-il le lien entre le nom de domaine <code>vote.&lt;YOUR_NAME&gt;.k8s.crafteo.io</code> et le Service Vote ?</li>
<li>Ajouter une configuration similaire pour <code>result.&lt;YOUR_NAME&gt;.k8s.crafteo.io</code> et le Service Result</li>
</ul>
<h2 id="ingress-controller"><a class="header" href="#ingress-controller">Ingress Controller</a></h2>
<p>Un Ingress ne fonctionne pas tout seul – il a besoin d'un Ingress Controller.</p>
<p>Traefik est actuellement déployé et agit comme Ingress Controller:</p>
<ul>
<li>Identifier le service LoadBalancer utilisé par Traefik</li>
<li>Est-il possible de déployer plusieurs Ingress Controllers ? </li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="externalname-services"><a class="header" href="#externalname-services">ExternalName Services</a></h1>
<p>Créer un service ExternalName pointant vers google.com:</p>
<pre><code>kubectl apply -f resources/externalname-service.yml 
</code></pre>
<ul>
<li>Lancer un shell dans le conteneur Vote avec <code>kubectl exec</code></li>
<li>Essayer de <code>curl https://external-test</code> et observer le résultat</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="concepts-avancés-sur-les-services"><a class="header" href="#concepts-avancés-sur-les-services">Concepts avancés sur les Services</a></h1>
<h2 id="endpoints-et-endpointslices"><a class="header" href="#endpoints-et-endpointslices">Endpoints et EndpointSlices</a></h2>
<p>Les Services utilisent des EndpointSlices (et Endpoints) en interne pour rediriger le trafic.</p>
<ul>
<li>Scaler le déploiement Vote à 3 replicas</li>
<li>Identifier les EndpointSlices et Endpoints créés pour le service Vote</li>
</ul>
<p>Historiquement, les Endpoints étaient utilisés pour lier les Services aux Pods. Aujourd'hui les EndpointSlices fournissent une API plus complète et remplaceront progressivement les Endpoints. Voir <a href="https://kubernetes.io/docs/concepts/services-networking/endpoint-slices/#motivation">la doc Kubernetes pour plus de détails</a></p>
<h2 id="headless-services"><a class="header" href="#headless-services">Headless Services</a></h2>
<p>Les Headless Services sont utilisés quand le load balancing n'est pas nécessaire. Les Headless services n'ont pas d'IP.</p>
<p>En utilisant des selectors, une requête DNS interne retournera directement toutes les IPs des pods existants.</p>
<ul>
<li>Scaler les déploiements Vote et Service à 3 replicas</li>
<li>Supprimer le service Vote</li>
<li>Mettre à jour le template du service Vote avec <code>spec.clusterIP: &quot;None&quot;</code></li>
<li>Re-déployer le service Vote</li>
<li>Lancer un shell dans le Pod Vote et observer le comportement DNS entre les services <code>vote</code> et <code>result</code>. 
<ul>
<li>Lancer un shell dans un container <code>kubectl exec -it ... sh</code> et ces commandes pour installer les outils DNS et checker la résolution:</li>
</ul>
</li>
</ul>
<pre><code># Installer des outils DNS à la volée pour tester
apt update &amp;&amp; apt install dns-utils

nslookup vote
nslookup result
</code></pre>
<h2 id="exposition-des-services-dans-les-pods"><a class="header" href="#exposition-des-services-dans-les-pods">Exposition des services dans les Pods</a></h2>
<p>Les services sont exposés aux Pods via des variables d'environnement.</p>
<ul>
<li>Lancer un shell dans le pod Vote et explorer les services disponibles
<ul>
<li>Utiliser <code>env</code> pour afficher toutes les variables d'environnement, et <code>env | grep</code> pour filtrer</li>
</ul>
</li>
<li>Trouver les variables d'environnement du service Result</li>
</ul>
<p>L'approche la plus courante est d'utiliser les enregistrements DNS pointant vers les Services.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="configmap"><a class="header" href="#configmap">ConfigMap</a></h1>
<p>Les ConfigMap sont utilisées pour monter des configurations ou des variables d'environnement dans les containers.</p>
<h2 id="variables-denvironnement"><a class="header" href="#variables-denvironnement">Variables d'environnement</a></h2>
<p>Le déploiement Database définit des variables d'environnement pour l'utilisateur et le mot de passe Postgres. Remplacer ces variables par celles définies dans la ConfigMap <code>resources/config/configmap-postgres-env.yml</code></p>
<ul>
<li>Créer la ConfigMap avec <code>kubectl apply</code></li>
<li>Mettre à jour le Deployment pour utiliser la ConfigMap afin de charger les variables d'environnement. Utiliser quelque chose comme ceci dans le template du Pod :</li>
</ul>
<pre><code class="language-yaml">    envFrom:
    - configMapRef:
        name: db-env
</code></pre>
<h2 id="fichiers-de-configs"><a class="header" href="#fichiers-de-configs">Fichiers de configs</a></h2>
<p>Utiliser la ConfigMap <code>resources/config/configmap-postgres-config.yml</code> pour monter une configuration Postgres personnalisée dans le container à <code>/etc/postgresql/postgresql.conf</code></p>
<ul>
<li>Équivalent de <code>docker run -v &quot;$PWD/my-postgres.conf&quot;:/etc/postgresql/postgresql.conf</code></li>
</ul>
<p>Utiliser quelque chose comme ceci dans le spec du Pod :</p>
<pre><code class="language-yaml">    spec:
      # [...]
      containers:
      - name: postgres
        # [...]
        # Utiliser un fichier de config personnalisé
        args: [ &quot;-c&quot;, &quot;config_file=/etc/postgresql/postgresql.conf&quot;]
        # Monter la config personnalisée dans le container
        volumeMounts:
        - mountPath: /etc/postgresql
          name: config-vol
      volumes:
      - name: config-vol
        configMap:
          name: db-config
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="secret"><a class="header" href="#secret">Secret</a></h1>
<p>Les Secrets sont similaires aux ConfigMaps, sauf qu'ils sont (théoriquement) chiffrés dans le Control Plane. Leur accès et utilisation doivent être protégés avec une autorisation appropriée dans le cluster.</p>
<p>Les Secrets nécessitent que leur configuration soit encodée en Base64. Utiliser une commande pour encoder une chaîne en Base64 :</p>
<pre><code>echo -n &quot;mypassword&quot; | base64
</code></pre>
<p><em>Note : Base64 n'est PAS un chiffrement. Il ne doit pas être utilisé pour protéger un secret. Base64 est utilisé car les secrets peuvent contenir des données binaires et leur représentation base64 peut être utilisée comme chaîne de caractères.</em></p>
<h1 id="utiliser-une-chaîne-en-clair-dans-un-secret-"><a class="header" href="#utiliser-une-chaîne-en-clair-dans-un-secret-">Utiliser une chaîne en clair dans un Secret ?</a></h1>
<p>Par défaut, les valeurs des secrets doivent être encodées en base64, ex :</p>
<pre><code class="language-yaml">data:
    # &quot;secretPassword&quot; en base 64
    password: c2VjcmV0UGFzc3dvcmQ=
</code></pre>
<p>Trouver un moyen d'utiliser des chaînes en clair au lieu de données encodées en base64 dans un manifest YAML Secret.</p>
<h1 id="variables-denvironnement-depuis-un-secret"><a class="header" href="#variables-denvironnement-depuis-un-secret">Variables d'environnement depuis un Secret</a></h1>
<p>Utiliser le Secret <code>resources/config/secret-postgres-env.yml</code> pour définir des variables d'environnement dans le Deployment Postgres</p>
<h1 id="fichiers-depuis-un-secret"><a class="header" href="#fichiers-depuis-un-secret">Fichiers depuis un Secret</a></h1>
<p>Copier la ConfigMap <code>resources/config/configmap-postgres-config.yml</code> et la transformer en Secret pour monter une configuration Postgres personnalisée dans le container à <code>/etc/postgresql/postgresql.conf</code></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="persistent-volume-claims"><a class="header" href="#persistent-volume-claims">Persistent Volume Claims</a></h1>
<p>Un Persistent Volume Claim (PVC) est une demande d'espace de stockage. En créant un PVC, tu indiques au cluster que tu as besoin d'un espace de stockage et le cluster va essayer de le fournir.</p>
<ul>
<li>Créer un PVC à partir de <code>resources/volumes/pvc.yml</code></li>
<li>Mettre à jour le déploiement Database pour attacher le PVC créé:</li>
</ul>
<pre><code class="language-yml">apiVersion: apps/v1
kind: Deployment
# [...]
spec:
  template:
    spec:
      containers:
      - name: postgres
        # [...]
        env:
        # postgres requiert que le dossier où les données sont créées soit vide
        # Utiliser le chemin par défaut /var/lib/postgresql/data provoquerait une erreur
        # car il contient un dossier &quot;lost.found&quot;, empêchant le serveur de s'initialiser
        # Utiliser cette astuce pour stocker les données dans un sous-dossier du volume créé
        - name: PGDATA
          value: &quot;/var/lib/postgresql/data/pg&quot;
          
        volumeMounts:
        - mountPath: /var/lib/postgresql/data
          name: db-data
      volumes:
      - name: db-data
        persistentVolumeClaim:
          claimName: db-data-pvc # Le nom doit correspondre au nom du PVC
</code></pre>
<ul>
<li>Trouver le Persistent Volume (PV) créé suite à ta demande de stockage via le PVC</li>
<li>Détruire et recréer le déploiement Database, vérifier que le PVC est resté intact</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="emptydir-volume"><a class="header" href="#emptydir-volume">emptyDir volume</a></h1>
<p><code>emptyDir</code> peut être utilisé dans différentes situations, comme partager des données entre les containers d'un même Pod. Par exemple pour créer une sauvegarde d'une base de données:</p>
<ul>
<li>Configurer un Pod avec 2 containers,  <code>postgres</code> (dump database) et <code>aws</code> (upload de données)</li>
<li>Partager un volume <code>/backup</code> entre les 2 containers d'un même Pod pour &quot;passer&quot; le dump de <code>postgres</code> à <code>aws</code></li>
</ul>
<p>Un CronJob de backup est présent dans <code>resources/volumes/cronjob.yml</code> mais il manque la configuration des Volumes. Adapter le template pour :</p>
<ul>
<li>Déclarer un volume <code>emptyDir</code> (utiliser <code>volumes:</code> au bon endroit)
<ul>
<li>Le volume doit être déclaré au niveau du Pod puis un point de montage effectué au niveau de chaque container</li>
</ul>
</li>
<li>Monter le volume sur le path <code>/backup</code> pour les deux containers</li>
<li>Créer le CronJob et le déclencher (créer un Job à partir du CronJob) avec
<pre><code class="language-sh">kubectl create job --from=cronjob/postgres-backup manual-backup
</code></pre>
</li>
<li>Observer le résultat </li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="helm--package-manager-pour-kubernetes"><a class="header" href="#helm--package-manager-pour-kubernetes">Helm : package manager pour Kubernetes</a></h1>
<p><a href="https://helm.sh/">Helm</a> est un &quot;package manager&quot; pour Kubernetes. Il utilise des templates YAML pour gérer les ressources Kubernetes et permet de partager des &quot;Charts&quot; publiques ou privées. </p>
<p>On trouve des charts Helm sur <a href="https://artifacthub.io/">Artifact Hub</a> ou via n'importe quel moteur de recherche.</p>
<p>Exemple d'utilisation :</p>
<pre><code class="language-sh">helm --help

# Avant d'installer un chart, il faut souvent ajouter un Repository
# Ajouter le repository &quot;bitnami&quot;
helm repo add bitnami https://charts.bitnami.com/bitnami

# On peut aussi chercher un chart Helm
# Chercher les charts &quot;redis&quot;
helm search repo redis

# Installer un chart Redis
# 'myredis' est le nom du Release (comme un nom de container pour une image)
# 'bitnami/redis' est le chart à installer
helm install myredis bitnami/redis
</code></pre>
<p>Ajouter un repository n'est pas toujours nécessaire, de nombreuses charts sont aujourd'hui installables via OCI comme:</p>
<pre><code class="language-sh">helm install my-redis oci://registry-1.docker.io/bitnamicharts/redis
</code></pre>
<h2 id="installer-un-chart-wordpress"><a class="header" href="#installer-un-chart-wordpress">Installer un chart Wordpress</a></h2>
<p>Trouver un chart Wordpress et l'installer. Essayer d'y accéder en externe avec un port-forward.</p>
<ul>
<li>Wordpress est un système de blog avec une base de données. Il fournit un frontend simple à atteindre depuis internet.</li>
<li>Chercher un chart Wordpress par n'importe quelle méthode</li>
</ul>
<p>Une fois terminé, désinstaller le release du chart Wordpress</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="helm-avancé-écrire-un-chart-helm"><a class="header" href="#helm-avancé-écrire-un-chart-helm">Helm avancé: écrire un chart Helm</a></h1>
<p>Le dossier <code>resources/helm/example-voting-app</code> contient un chart Helm avec des manifests YAML templatisés.</p>
<h2 id="déploiement-mise-à-jour-et-values"><a class="header" href="#déploiement-mise-à-jour-et-values">Déploiement, mise à jour et values</a></h2>
<p>Utiliser la commande <code>helm install</code> pour déployer un release du chart Example Voting App.</p>
<ul>
<li>Vérifier le fonctionnement via un port-forward</li>
<li>Explorer les templates YAML pour comprendre le mécanisme de templating et le lien avec <code>values.yml</code></li>
</ul>
<p>Il est possible d'override <code>values.yml</code> avec des fichiers de configuration externes, typiquement par environnement.</p>
<ul>
<li>Mettre à jour le release pour surcharger les valeurs par défaut avec <code>resources/helm/values/dev.yml</code></li>
</ul>
<h2 id="gestion-des-secrets"><a class="header" href="#gestion-des-secrets">Gestion des Secrets</a></h2>
<p>Helm ne fournit pas de mécanisme natif pour gérer les secrets. Un pattern courant est de référencer un secret externe (non géré par le chart).</p>
<p>Mettre à jour le release Helm en utilisant les valeurs de <code>resources/helm/values/prod.yml</code>.</p>
<ul>
<li>Cette configuration référence un secret externe, <strong>il faut le créer soi-même</strong></li>
<li>Explorer le contenu du chart pour comprendre le mécanisme sous-jacent</li>
</ul>
<p>Il existe aussi des <a href="https://helm.sh/docs/topics/plugins/">plugins Helm</a> permettant la gestion des secrets comme <a href="https://github.com/jkroepke/helm-secrets"><code>helm-secrets</code></a></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="kustomize"><a class="header" href="#kustomize">Kustomize</a></h1>
<p>Kustomize est un outil intégré à <code>kubectl</code> pour du déploiement multi-environnements et la gestion de configuration.</p>
<p>Voir la <a href="https://kubectl.docs.kubernetes.io">documentation officielle de Kustomize</a> et la <a href="https://kubectl.docs.kubernetes.io/references/kustomize/">documentation de référence</a></p>
<h2 id="kustomize-example-voting-app"><a class="header" href="#kustomize-example-voting-app">Kustomize Example Voting App</a></h2>
<p>Un déploiement d'application avec Kustomize nécessite:</p>
<ul>
<li>Une <em>base</em> avec la config YAML (Deployment, Services, etc.) et un <code>kustomization.yml</code> listant les ressources et options souhaitées</li>
<li>Un ou plusieurs overrides (typiquement par environnement, mais pas forcément)</li>
</ul>
<p>Pour Kustomizer Example Voting App :</p>
<ul>
<li>Copier <code>resources/kustomize/base-kustomization.yml</code> dans <code>base/kustomization.yml</code>. Ce fichier référence toutes les ressources de l'Example Voting App.</li>
<li>Utiliser l'un des dossiers <code>resources/kustomize/dev</code> ou <code>resources/kustomize/prod</code> avec <code>kubectl</code>. Chacun référence sa base via un chemin relatif, ex : <code>resources: [ &quot;../../../base&quot; ]</code></li>
</ul>
<pre><code class="language-sh">#
# Attention au -k
#
# Comme kubectl apply -f mais va lire depuis dev et toutes les ressources référencées (y compris la base)
kubectl apply -n &lt;YOU&gt; -k resources/kustomize/dev
</code></pre>
<p>Observer le résultat selon le contenu des fichiers <code>kustomization.yml</code> de dev et base, en particulier <code>commonLabels</code></p>
<h2 id="définir-le-namespace"><a class="header" href="#définir-le-namespace">Définir le namespace</a></h2>
<p>Créer un namespace avec votre nom préfixé par <code>-kustomize</code>, ex : <code>YOU-kustomize</code>.</p>
<p>Mettre à jour le <code>kustomization.yml</code> de dev pour définir le namespace <code>YOU-kustomize</code> et appliquer sans flag <code>-n xxx</code>, ex :</p>
<pre><code class="language-sh">kubectl apply -k resources/kustomize/dev
</code></pre>
<p>Que sont devenues les ressources dans l'ancien namespace ?</p>
<h2 id="patches-et-générateurs-configmapsecret"><a class="header" href="#patches-et-générateurs-configmapsecret">Patches et générateurs ConfigMap/Secret</a></h2>
<p>Utiliser un <a href="https://kubectl.docs.kubernetes.io/guides/config_management/secrets_configmaps/">ConfigMap generator</a> pour créer une ConfigMap à partir du fichier <code>postgresql.conf</code></p>
<p>Utiliser un <a href="https://kubectl.docs.kubernetes.io/guides/config_management/secrets_configmaps/">Secret generator</a> pour créer un Secret à partir du fichier <code>postgres-secret.properties</code></p>
<ul>
<li>Il faut des options supplémentaires pour considérer ce fichier comme des variables d'environnement</li>
</ul>
<p>Surcharger le Deployment avec un <em>patch</em> pour s'assurer que les valeurs du Secret et de la ConfigMap sont utilisées à la place de celles définies dans la base.</p>
<ul>
<li>Utiliser <code>db-deployment-patch.yml</code> et mettre à jour <code>kustomization.yml</code></li>
</ul>
<h2 id="adapter-une-autre-kustomization"><a class="header" href="#adapter-une-autre-kustomization">Adapter une autre Kustomization</a></h2>
<p>Reproduire les changements de la Kustomization <code>dev</code> dans la Kustomization <code>prod</code> et déployer les deux dans le même namespace. </p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="ingress-avec-tls-https"><a class="header" href="#ingress-avec-tls-https">Ingress avec TLS (HTTPS)</a></h1>
<p><code>resources/https</code> contient une ressource Ingress configurée avec TLS (HTTPS) et un Certificate <a href="https://github.com/cert-manager/cert-manager">Cert Manager</a>.</p>
<p>Déployer cet Ingress et le Certificate avec Example Voting App</p>
<ul>
<li>Vérifier que Example Voting App est déployée</li>
<li>Déployer les ressources dans <code>resources/https</code>. <strong>Remplacer <code>&lt;YOUR_NAME&gt;</code> dans les YAML par votre nom.</strong></li>
<li>Observer la création des ressources Ingress et Certificate</li>
<li>Tester la fonctionnalité. L'Ingress doit être accessible de l'extérieur. </li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="pod-resources--requests"><a class="header" href="#pod-resources--requests">Pod Resources / Requests</a></h1>
<p>Les Pods peuvent avoir des <a href="https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/">requests et limits de ressources</a>, par exemple:</p>
<pre><code class="language-yaml">spec:
  template:
    spec:
      containers:
      - name: # ...
        resources:
          requests:
            cpu: 1
            memory: &quot;1Gi&quot;
          limits:
            cpu: 2
            memory: 2Gi
</code></pre>
<p>Affecter des ressources à vos Pods :</p>
<ul>
<li>Mettre à jour les Pods Vote pour demander 1 CPU et 1024Mi de mémoire</li>
<li>Quelles unités peut-on utiliser pour spécifier les ressources ? Quelle différence entre <code>128M</code> et <code>128Mi</code> ?</li>
</ul>
<p>Requests et limits affectent le scheduling différemment. Mettre les ressources des Pods Vote comme ci-dessous et scaler à 10 replicas. Observer le résultat.</p>
<pre><code class="language-yml">resources:
  requests:
    cpu: 1m
    memory: 2Mi
  limits:
    cpu: 1
    memory: 1Gi
</code></pre>
<p>Mettre ensuite les ressources comme ci-dessous, scaler à 10 replicas et observer le résultat.</p>
<pre><code class="language-yml">resources:
  requests:
    cpu: 250m
    memory: 256Mi
  limits:
    cpu: 1
    memory: 1Gi
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="scalabilité-horizontale-et-horizontal-pod-autoscaler-hpa"><a class="header" href="#scalabilité-horizontale-et-horizontal-pod-autoscaler-hpa">Scalabilité horizontale et Horizontal Pod Autoscaler (HPA)</a></h1>
<p>Configurer un Horizontal Pod Autoscaler (HPA) pour scaler automatiquement les Pods Vote lorsque leur charge CPU est élevée. On utilisera un Pod Auto Voter pour simuler une charge CPU (beaucoup de votes !)</p>
<p>Mettre à jour le Deployment Vote pour spécifier les ressources et requests:</p>
<pre><code class="language-yaml">        resources:
          requests:
            cpu: 50m
            memory: 128Mi
          limits:
            cpu: 100m
            memory: 256Mi
</code></pre>
<p>Déployer Auto Voter : <code>kubectl apply -f resources/scaling/auto-voter.yml</code></p>
<ul>
<li>C'est une simple boucle bash qui envoie des requêtes POST au service Vote.</li>
<li>Observer la charge CPU de Vote avec la commande <code>kubectl top</code> (CPU idle ~2m, devrait passer à ~50m). Exemple :</li>
</ul>
<pre><code class="language-sh">watch kubectl top pod vote-xxx
</code></pre>
<p>Déployer le HPA dans <code>resources/scaling/hpa.yml</code> et observer la scalabilité</p>
<ul>
<li>Comment le HPA sait-il quand scaler un pod ?</li>
</ul>
<p>Supprimer Auto Voter et observer le HPA réduire le nombre de pods. </p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="scalabilité-verticale-et-cluster-autoscaler"><a class="header" href="#scalabilité-verticale-et-cluster-autoscaler">Scalabilité verticale et Cluster Autoscaler</a></h1>
<p>Le Cluster Autoscaler observe en continu l'état du cluster et ajoute ou retire des Nodes selon le besoin. Il utilise les requests de ressources et la capacité de scheduling des Pods pour décider d'ajouter ou retirer des Nodes. </p>
<p>Configurer le deployment Vote pour avoir les ressources suivantes :</p>
<pre><code class="language-yaml">        resources:
          requests:
            cpu: 500m
            memory: 512Mi
          limits:
            cpu: 1000m
            memory: 1024Mi
</code></pre>
<p>Scaler ensuite le deployment Vote à 20 Pods</p>
<pre><code class="language-sh">kubectl scale deployment vote --replicas 20
</code></pre>
<p>Observer le comportement des Pods :</p>
<ul>
<li>Les Pods sont en Pending</li>
<li>De nouveaux nodes sont ajoutés pour accueillir les nouveaux Pods (jusqu'à un certain maximum)</li>
</ul>
<p>Revenir à 1 replica</p>
<ul>
<li>Observer la suppression des nodes</li>
</ul>
<p>Identifier le Cluster Autoscaler dans <code>kube-system</code> responsable de l'autoscaling. </p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="liveness-readiness-et-startup-probes"><a class="header" href="#liveness-readiness-et-startup-probes">Liveness, Readiness et Startup Probes</a></h1>
<p><a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/">Liveness, Readiness et Startup Probes</a> définissent des comportements pour gérer le routage du traffic réseau et auto-réparer les Pods et containers en cas de problèmes (comme une boucle infini ou un blocage d'une application qui n'a pas fait crasher le pod)</p>
<h2 id="liveness-probe"><a class="header" href="#liveness-probe">Liveness probe</a></h2>
<p>La liveness probe vérifie régulièrement l'état du container et le redémarre en cas d'échec. Note : c'est le <em>container</em> qui est redémarré (tué et recréé), pas le Pod.</p>
<p>Ajouter une Liveness Probe sur le container Vote, par exemple :</p>
<pre><code class="language-yml">        livenessProbe:
         httpGet:
           path: /
           port: 80
         periodSeconds: 2 # Normalement ~10s, ici plus court pour observer le comportement
</code></pre>
<p>Appliquer les changements.</p>
<p>Pour tester le comportement, overrider la commande du container Vote:</p>
<pre><code class="language-yaml">        command: [&quot;sleep&quot;, &quot;infinity&quot;]
</code></pre>
<p>Cela va faire échouer la Liveness probe car le serveur Vote ne démarre pas, <code>GET /</code> ne fonctionnera donc pas. Appliquer et observer le comportement (<code>kubectl describe</code> pour voir les events).</p>
<p>Mettre à jour la liveness probe pour démarrer 10s après le démarrage du container, pour fail après 8 tentatives.</p>
<h2 id="readiness-probe"><a class="header" href="#readiness-probe">Readiness probe</a></h2>
<p>Ajouter une Readiness Probe sur le container Vote (garder la Liveness Probe). Utiliser une méthode similaire pour tester le comportement.</p>
<p>Décrire le service Vote et vérifier qu'aucun trafic n'est routé vers un Pod non-Ready (dont la Readiness probe échoue).</p>
<h2 id="startup-probe"><a class="header" href="#startup-probe">Startup probe</a></h2>
<p>Ajouter une Startup Probe sur le container Vote (garder Liveness et Readiness). Utiliser une méthode similaire pour tester le comportement.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="pod-disruption-budget-pdb"><a class="header" href="#pod-disruption-budget-pdb">Pod Disruption Budget (PDB)</a></h1>
<p>Les Pod Disruption Budget (PDB) servent à spécifier un nombre de Pods pouvant devenir indisponibles lors d'une mise à jour. Si le PDB ne peut pas être respecté, la mise à jour d'un Deployment ou autre object échouera ou restera bloqué. C'est un méchanisme de sécurité pour empêcher les downtimes involontaires. Voir <a href="https://kubernetes.io/docs/tasks/run-application/configure-pdb/">la doc officielle</a></p>
<p>Mettre à jour le Deployment Vote pour avoir 5 replicas, une Readiness Probe et préférer déployer les Pods sur un seul node, ex :</p>
<pre><code class="language-yml">apiVersion: apps/v1
kind: Deployment
# ...
spec:
  replicas: 5
  template:
    spec:
      affinity:
        nodeAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - weight: 100
              preference:
                matchExpressions:
                  - key: &quot;kubernetes.io/hostname&quot;
                    operator: In
                    values:
                      - &quot;ip-192-168-1-4.eu-west-3.compute.internal&quot;
      containers:
      - name: vote
        # ...
        readinessProbe:
          httpGet:
            path: /
            port: 80
          initialDelaySeconds: 5
</code></pre>
<p>Créer un PDB pour Vote pour permettre max 1 Pod indisponible, par exemple :</p>
<pre><code class="language-yml">apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: vote-pdb
spec:
  minAvailable: 4
  selector:
    matchLabels:
      app: vote
</code></pre>
<p>Drainer ensuite le Node sur lequel les Pods sont déployés pour observer le comportement.</p>
<pre><code class="language-sh"># Exemple de commande pour drainer un node
# Remplacer par le nom de votre node
kubectl drain ip-192-168-1-160.eu-west-3.compute.internal --delete-emptydir-data=true --ignore-daemonsets=true
</code></pre>
<p>Reproduire avec <code>maxUnavailable: 2</code> au lieu de <code>minAvailable: 1</code></p>
<p>Que se passe-t-il si on met une valeur en pourcentage et que le nombre de Pods n'est pas rond ? (ex : minAvailable à 50% sur 5 replicas)</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="contraintes-de-scheduling-introduction"><a class="header" href="#contraintes-de-scheduling-introduction">Contraintes de Scheduling: introduction</a></h1>
<p>Les contraintes de scheduling des Pods permettent de spécifier comment un Pod sera scheduler sur un Node. Voir <a href="https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity">la documentation officielle</a></p>
<h2 id="node-selector-affecter-des-pods-via-les-labels-node"><a class="header" href="#node-selector-affecter-des-pods-via-les-labels-node">Node selector: affecter des Pods via les labels Node</a></h2>
<p>Les Nodes Kubernetes ont des labels comme les autres objets Kubernetes. On peut obtenir les labels d'un Node avec <code>get</code> ou <code>describe</code>.</p>
<pre><code class="language-sh">kubectl get node some-node-name -o yaml
</code></pre>
<pre><code class="language-yaml">apiVersion: v1
kind: Node
metadata:
  name: some-node-name

  # Ces labels peuvent être utilisés par les Pods pour sélectionner les Nodes
  # Comme les Services utilisent labelSelector pour sélectionner les Pods
  labels:
    beta.kubernetes.io/arch: amd64
    beta.kubernetes.io/os: linux
    kubernetes.io/hostname: some-node-name
    topology.kubernetes.io/region: us-central1
    topology.kubernetes.io/zone: us-central1-a

# ...
</code></pre>
<p>Utiliser <code>nodeSelector</code> pour affecter les Pods du Deployment Vote à un Node donné via le label <code>topology.kubernetes.io/zone</code>.</p>
<ul>
<li>Utiliser <code>kubectl describe node</code> ou <code>kubectl get node -o yaml</code> pour identifier les labels des Nodes</li>
<li>Mettre à jour le Deployment Vote pour définir <code>nodeSelector</code> et appliquer</li>
</ul>
<h2 id="affecter-un-pod-à-un-node-spécifique"><a class="header" href="#affecter-un-pod-à-un-node-spécifique">Affecter un Pod à un Node spécifique</a></h2>
<p>Utiliser le champ <code>nodeName</code> d'un Pod pour l'affecter à un Node précis.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="topology-spread-constraint"><a class="header" href="#topology-spread-constraint">Topology Spread Constraint</a></h1>
<p><a href="https://kubernetes.io/docs/concepts/scheduling-eviction/topology-spread-constraints/">Topology Spread Constraints</a> permettent de répartir la charge sur plusieurs domaines (zones, régions, etc.) topologiques. C'est utile pour <strong>améliorer la haute disponibilité et la résilience</strong> en répartissant les Pods sur plusieurs zones, régions, etc.</p>
<p><code>topologySpreadConstraints</code> utilise <code>topologyKey</code> pour répartir les Pods sur les domaines correspondant à la clé, par exemple :</p>
<pre><code class="language-yaml">    topologySpreadConstraints:
      # Différence maximale entre deux domaines,
      # ex. un domaine ne peut pas avoir plus d'1 Pod qu'un autre
      - maxSkew: 1

        # Clé de topologique utilisée pour définir le domaine
        # Les Pods seront répartis sur différentes zones
        topologyKey: topology.kubernetes.io/zone
        
        # Comportement si la contrainte ne peut pas être respectée
        whenUnsatisfiable: DoNotSchedule

        # Seuls les Pods correspondant à ces labels seront contraints
        labelSelector:
          matchLabels:
            app: vote

</code></pre>
<p>Mettre à jour le Deployment Vote pour équilibrer la charge dans la région courante.</p>
<p>Quel est le comportement de <code>topologySpreadConstraints</code> par rapport à Node/Pod Affinity ? </p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="node-affinity-et-anti-affinity"><a class="header" href="#node-affinity-et-anti-affinity">Node Affinity et Anti-Affinity</a></h1>
<p>Affinity et Anti-affinity existent en deux variantes:</p>
<blockquote>
<p><code>requiredDuringSchedulingIgnoredDuringExecution</code> : le scheduler ne peut pas programmer le Pod si la règle n'est pas respectée. Fonctionne comme <code>nodeSelector</code>, mais avec une syntaxe plus expressive.
<code>preferredDuringSchedulingIgnoredDuringExecution</code> : le scheduler essaie de trouver un node qui respecte la règle. Si aucun node ne correspond, le Pod est quand même programmé.</p>
</blockquote>
<p>Source : <a href="https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#node-affinity">doc officielle</a></p>
<p>En résumé :</p>
<ul>
<li><code>requiredDuringSchedulingIgnoredDuringExecution</code> = <strong>contrainte forte</strong> : le Pod ne sera pas programmé si la contrainte n'est pas respectée</li>
<li><code>preferredDuringSchedulingIgnoredDuringExecution</code> = <strong>contrainte souple</strong> : le Pod est préféré sur les nodes qui respectent la contrainte, mais sera programmé ailleurs si besoin</li>
</ul>
<p>On peut définir les deux en même temps pour des comportements complexes.</p>
<p>Exemple: Node Affinity qui exige qu'un Pod soit schedulé dans une zone spécifique, comme un Node Selector:</p>
<pre><code class="language-yml">spec:
  template:
    spec:
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
             - matchExpressions:
                 - key: &quot;topology.kubernetes.io/zone&quot;
                   operator: In
                   values:
                     - &quot;eu-west-3b&quot;
</code></pre>
<p>Mettre à jour le Deployment Vote pour :</p>
<ul>
<li>Préférer programmer les Pods dans la zone <code>eu-west-3b</code> (utiliser <code>preferred</code> au lieu de <code>required</code>)</li>
<li>Si le Deployment ou les Pods sont bloqués, supprimer le Deployment et le recréer</li>
</ul>
<p>Appliquer les changements et observer le comportement du scheduling. Supprimer tous les Pods Vote et observer leur scheduling.</p>
<p>Ajouter une seconde règle pour que le Pod :</p>
<ul>
<li>Préfère la zone <code>eu-west-3b</code> avec un <code>weight</code> de <code>100</code></li>
<li>Préfère la zone <code>eu-west-3a</code> avec un <code>weight</code> de <code>50</code></li>
<li>Observer le résultat</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="pod-affinity-et-anti-affinity"><a class="header" href="#pod-affinity-et-anti-affinity">Pod Affinity et Anti-affinity</a></h1>
<blockquote>
<p>Pod affinity et Anti-affinity permettent de contraindre sur quels nodes vos Pods peuvent être schedulés en fonction des labels des Pods déjà présents sur ce node, au lieu des labels du node. Bien que complexe à utiliser, ils permettent des schémas de scheduling avancés. </p>
</blockquote>
<p>Source : <a href="https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity">doc officielle</a></p>
<h2 id="répartir-les-pods-sur-les-nodes"><a class="header" href="#répartir-les-pods-sur-les-nodes">Répartir les Pods sur les Nodes</a></h2>
<p>Contexte: on souhaite répartir les Pods Vote sur les nodes pour équilibrer la charge et améliorer la redondance et la résilience.</p>
<p>Exemple: scheduler les Pods Vote sur un Node qui n'a pas déjà un Pod Vote via une Pod Anti-affinity:</p>
<pre><code class="language-yml">      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          # Groupe les Nodes par Hostname (donc 1 noeud par groupe)
          # Selectionne un groupe (donc un Node) qui n'a pas de Pod matchant le label &quot;app=vote&quot;
          - topologyKey: &quot;kubernetes.io/hostname&quot;
            labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - vote
            namespaceSelector:
              matchExpressions:
              - key: kubernetes.io/metadata.name
                operator: In
                values:
                  - &lt;YOUR NAME&gt;
</code></pre>
<p>Appliquer au Deployment Vote et observer le résultat.</p>
<p>Cependant, si chaque Node a déjà un Pod Vote, cette contrainte empêchera de programmer de nouveaux Pods Vote. Modifier la contrainte pour utiliser <code>preferredDuringSchedulingIgnoredDuringExecution</code> afin de répartir les Pods Vote sur les Nodes sans bloquer: <code>preferred</code> vs.  <code>required</code></p>
<h2 id="déployer-des-pods-sur-un-même-node"><a class="header" href="#déployer-des-pods-sur-un-même-node">Déployer des Pods sur un même Node</a></h2>
<p>Contexte: pour de meilleures performances, on veut que les Pods Redis soient au plus proche des Pods Vote, en préférant répartir les Pods Redis sur les Nodes et en préférant que ces Pods Vote soient déployés sur un Node qui a déjà un Pod Redis (pour réduire la latence réseau en contactant Redis).</p>
<p>Pod anti-affinity :</p>
<ul>
<li>Configurer le Deployment Redis pour <strong>préférer</strong> être programmé sur un Node sans Pod Redis déjà présent.</li>
</ul>
<p>Pod affinity :</p>
<ul>
<li>Configurer le Deployment Vote pour <strong>exiger</strong> d'être programmé sur un Node qui a déjà un Pod Redis (pour optimiser la communication) et <strong>préférer</strong> un Node qui n'a pas déjà un Pod Vote.</li>
</ul>
<p>Jouer avec les replicas sur les Deployments Redis et Vote pour observer les résultats.</p>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->


                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">

            </nav>

        </div>




        <script>
            window.playground_copyable = true;
        </script>


        <script src="elasticlunr.min.js"></script>
        <script src="mark.min.js"></script>
        <script src="searcher.js"></script>

        <script src="clipboard.min.js"></script>
        <script src="highlight.js"></script>
        <script src="book.js"></script>

        <!-- Custom JS scripts -->

        <script>
        window.addEventListener('load', function() {
            window.setTimeout(window.print, 100);
        });
        </script>

    </div>
    </body>
</html>
